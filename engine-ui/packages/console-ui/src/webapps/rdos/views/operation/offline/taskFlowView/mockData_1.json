{
    "code": 1,
    "message": null,
    "data": {
        "batchTask": {
            "id": 804,
            "gmtCreate": 1535614204000,
            "gmtModified": 1537155710000,
            "isDeleted": 0,
            "tenantId": 129,
            "projectId": 77,
            "name": "job_dwd_ord_trd_d",
            "taskType": 0,
            "computeType": 1,
            "engineType": 1,
            "sqlText": "CREATE TABLE if NOT EXISTS dwd_ord_trd_d (\norderId             STRING          COMMENT '订单ID'\n,tenantId           STRING          COMMENT '租户ID'\n,shopId             STRING          COMMENT '门店ID'\n,customerId         STRING          COMMENT '顾客ID'\n,memberId           STRING          COMMENT '会员ID'\n,orderTime\t        STRING\t        COMMENT '下单时间'\n,paymentTime\t      STRING\t        COMMENT '结账时间'\n,deliverType\t      BIGINT\t        COMMENT '就餐类型(1:内用;2:外送;3:自提;4:外带;)'\n,sourceChild\t      BIGINT\t        COMMENT '订单子来源(1:ANDROID收银终端;2:IPAD自助终端;3:IPAD收银终端;31:微信官微;32:微信商微;33:微信快捷支付;41:百度外卖;51:百度直达号;61:百度糯米点菜;71:百度地图;81:呼叫中心;91:必胜客自助;92:味千;111:商户官网;131:loyal;141:OnMobile客如云;142:OnMobile os;151:熟客;161:饿了吗;171:点评正餐;181:美团外卖;191:安卓自助;)'\n,tradeNo            STRING          COMMENT '订单编号，门店下唯一'\n,tradePayStatus\t    BIGINT\t        COMMENT '支付状态(1:未支付;2:支付中;3:已支付;4:退款中;5:已退款;6:退款失败;7:预支付;8:等待退款;9:支付失败;)'\n,tradeType          STRING          COMMENT '交易类型 1:SELL:售货 2:REFUND:退货 3:SPLIT:拆单 4:反结账(已收款退货时新生成的订单) 5:反结账退货(已收款退货时新生成的反向订单,金额是销货单的负数)'\n,tradeStatus\t      BIGINT\t        COMMENT '订单状态(1:未处理;2:挂单;3:已确认;4:已完成(全部支付);5:已退货;6:已作废;7:已拒绝;8:已取消;10:已反结账;)'\n-- ,payModelId\t      BIGINT\t        COMMENT '支付方式: -1:会员卡余额,-2:优惠券,-3:现金,-4:银行卡,-5:微信支付,-6:支付宝,-7:百度钱包,-8:百度直达号,-9:积分抵现,-10:百度地图,-11:银联刷卡,-12:百糯到店付,13:百度外卖,-14:饿了么,-15:实体卡支付,-16:大众点评,-17:美团外卖,-18:点评团购券,-19:点评闪惠,-20:临时卡余额,-21:糯米点菜,-22:第三方C端,-23:美团闪惠,-24:美团团购卷,-25:钱包生活,-26:百度糯米团购券,-27:乐富支付'\n-- ,paySource\t        BIGINT\t        COMMENT '支付来源，1：Cashier 2：Kiosk 3：百度外卖 4：百度糯米 5： 百度地图 6： Loyal（短信储值） 7：OnMobile 8：快捷支付 9：熟客 10：portal 11:饿了么 12:大众点评 13：loyal后台 14：美团外卖 15:微信'\n,thirdServiceCharge\tDOUBLE \t        COMMENT '服务费'\n,thirdSubsidies\t    DOUBLE       \t  COMMENT '补贴'\n,shopActualAmount   DOUBLE          COMMENT '商户实收金额'\n,tradeAmount        DOUBLE          COMMENT '订单原始金额'\n,custActualPay      DOUBLE          COMMENT '顾客实付金额'\n,custShouldPay\t    DOUBLE\t        COMMENT '顾客应付金额'\n,dishTotalAmount\t  DOUBLE\t        COMMENT '商品总额'\n,extraAmount\t      DOUBLE\t        COMMENT '附加金额'\n,privilegeAmount\t  DOUBLE          COMMENT '优惠金额'\n,molinAmount\t      DOUBLE      \t  COMMENT '抹零金额'\n,overFlowAmt\t      DOUBLE\t        COMMENT '四舍五入'\n,tradeOverFlow\t    DOUBLE      \t  COMMENT '溢收金额'\n) COMMENT '日订单交易信息明细表'\nPARTITIONED BY (ds STRING COMMENT '时间分区') STORED AS orc LIFECYCLE 100;\n\n\nset spark.sql.shuffle.partitions=1;\n\n\nINSERT OVERWRITE TABLE dwd_ord_trd_d PARTITION(ds = '${bdp.system.bizdate}')\nSELECT DISTINCT\n  coalesce(t2.orderId, t1.orderid)                       AS orderid,\n  coalesce(t2.tenantId, t1.tenantId)                     AS tenantId,\n  coalesce(t2.shopId, t1.shopId)                         AS shopId,\n  coalesce(t2.customerId, t1.customerId)                 AS customerId,\n  coalesce(t2.memberId, t1.memberId)                     AS memberId,\n  coalesce(t2.orderTime, t1.orderTime)                   AS orderTime,\n  coalesce(t2.paymentTime, t1.paymentTime)               AS paymentTime,\n  coalesce(t2.deliverType, t1.deliverType)               AS deliverType,\n  coalesce(t2.sourceChild, t1.sourceChild)               AS sourceChild,\n  coalesce(t2.tradeNo, t1.tradeNo)                       AS tradeNo,\n  coalesce(t2.tradePayStatus, t1.tradePayStatus)         AS tradePayStatus,\n  coalesce(t2.tradeType, t1.tradeType)                   AS tradeType,\n  coalesce(t2.tradeStatus, t1.tradeStatus)               AS tradeStatus,\n  coalesce(t2.thirdServiceCharge, t1.thirdServiceCharge) AS thirdServiceCharge,\n  coalesce(t2.thirdSubsidies, t1.thirdSubsidies)         AS thirdSubsidies,\n  coalesce(t2.shopActualAmount, t1.shopActualAmount)     AS shopActualAmount,\n  coalesce(t2.tradeAmount, t1.tradeAmount)               AS tradeAmount,\n  coalesce(t2.custActualPay, t1.custActualPay)           AS custActualPay,\n  coalesce(t2.custShouldPay, t1.custShouldPay)           AS custShouldPay,\n  coalesce(t2.dishTotalAmount, t1.dishTotalAmount)       AS dishTotalAmount,\n  coalesce(t2.extraAmount, t1.extraAmount)               AS extraAmount,\n  coalesce(t2.privilegeAmount, t1.privilegeAmount)       AS privilegesTotal,\n  coalesce(t2.molinAmount, t1.molinAmount)               AS molinAmount,\n  coalesce(t2.overFlowAmt, t1.overFlowAmt)               AS overFlowAmt,\n  coalesce(t2.tradeOverFlow, t1.tradeOverFlow)           AS tradeOverFlow\nFROM\n  (SELECT\n     orderId,\n     tenantId,\n     shopId,\n     customerId,\n     memberId,\n     orderTime,\n     paymentTime,\n     deliverType,\n     sourceChild,\n     tradeNo,\n     tradePayStatus,\n     tradeType,\n     tradeStatus,\n     thirdServiceCharge,\n     thirdSubsidies,\n     shopActualAmount,\n     tradeAmount,\n     custActualPay,\n     custShouldPay,\n     dishTotalAmount,\n     extraAmount,\n     privilegeAmount,\n     molinAmount,\n     overFlowAmt,\n     tradeOverFlow\n   FROM dwd_ord_trd_d WHERE ds = '${key_2d}') t1 FULL OUTER JOIN\n  (SELECT\n     o.id                 AS orderId,\n     o.tenantId           AS tenantId,\n     shop.id              AS shopId,\n     oc.customerId        AS customerId,\n     oc.memberId          AS memberId,\n     o.orderTime          AS orderTime,\n     o.paymentTime        AS paymentTime,\n     o.deliverType        AS deliverType,\n     o.sourceChild        AS sourceChild,\n     o.tradeNo            AS tradeNo,\n     o.tradePayStatus     AS tradePayStatus,\n     o.tradeType          AS tradeType,\n     o.tradeStatus        AS tradeStatus,\n     o.thirdServiceCharge AS thirdServiceCharge,\n     o.thirdSubsidies     AS thirdSubsidies,\n     o.receivedAmount     AS shopActualAmount,\n     o.tradeAmount        AS tradeAmount,\n     o.custActualPay      AS custActualPay,\n     o.custShouldPay      AS custShouldPay,\n     o.dishTotalAmount    AS dishTotalAmount,\n     o.extrachargeTotal   AS extraAmount,\n     o.privilegesTotal    AS privilegeAmount,\n     o.molinAmount        AS molinAmount,\n     o.overFlowAmt        AS overFlowAmt,\n     o.tradeOverFlow      AS tradeOverFlow\n   FROM ods_nrs_sync_keruyun_order o\n     LEFT OUTER JOIN ods_nrs_sync_keruyun_order_customer oc ON o.id = oc.orderId AND oc.ds = '${bdp.system.bizdate}'\n     LEFT OUTER JOIN ods_nrs_tenant_shop shop\n       ON o.tenantId = shop.tenantId AND o.shopId = shop.keruyun_shop_id AND shop.ds = '${bdp.system.bizdate}'\n   WHERE o.ds = '${bdp.system.bizdate}') t2\n    ON t1.tenantId = t2.tenantId AND t1.shopId = t2.shopId AND t1.orderId = t2.orderId AND t1.tradeNo = t2.tradeNo AND\n       t1.customerId = t2.customerId;\n\n",
            "taskParams": "##Number of CPU cores need driver program is running\n##driver.cores=1\n\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\n##driver.maxResultSize=1g\n\n##Driver number memory used by a process\n##driver.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##executor.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##logConf=spark.logConf",
            "scheduleConf": "{\"selfReliance\":0,\"min\":\"0\",\"hour\":\"2\",\"periodType\":\"2\",\"beginDate\":\"2001-01-02\",\"endDate\":\"2121-01-01\"}",
            "periodType": null,
            "scheduleStatus": 2,
            "submitStatus": 1,
            "modifyUserId": 90,
            "createUserId": 90,
            "ownerUserId": 90,
            "version": 67,
            "nodePid": 1163,
            "taskDesc": "",
            "mainClass": "",
            "exeArgs": "",
            "flowId": 0,
            "createUser": {
                "id": 90,
                "gmtCreate": 1535092930000,
                "gmtModified": 1535092930000,
                "isDeleted": 0,
                "userName": "yizhou@dtstack.com",
                "phoneNumber": null,
                "dtuicUserId": 6999,
                "email": "",
                "status": 0,
                "defaultProjectId": null
            },
            "modifyUser": null,
            "ownerUser": null,
            "taskPeriodId": 2,
            "taskPeriodType": "天任务",
            "nodePName": null,
            "readWriteLockVO": null,
            "userId": null,
            "lockVersion": null,
            "taskVariables": null,
            "forceUpdate": false,
            "subNodes": null,
            "relatedTasks": null,
            "createModel": 0,
            "operateModel": 0,
            "pythonVersion": 0,
            "learningType": 0,
            "input": null,
            "output": null,
            "options": null,
            "flowName": null,
            "syncModel": 0,
            "increColumn": null,
            "taskVOS": null,
            "subTaskVOS": null,
            "resourceList": null,
            "refResourceList": null,
            "taskVersions": null,
            "cron": "0 0 2 * * ?"
        },
        "id": 278544,
        "gmtCreate": 1547647311000,
        "gmtModified": 1547647311000,
        "isDeleted": 0,
        "tenantId": null,
        "projectId": null,
        "jobId": "a9fa1f93",
        "jobKey": "cronTrigger_804_20190117020000",
        "jobName": "cronJob_job_dwd_ord_trd_d_20190117020000",
        "status": 18,
        "taskId": 804,
        "createUserId": 90,
        "type": 0,
        "businessDate": "2019-01-16 ",
        "cycTime": "2019-01-17 02:00:00",
        "execStartTime": null,
        "execEndTime": null,
        "execTime": null,
        "execStartDate": null,
        "execEndDate": null,
        "taskPeriodId": 2,
        "taskPeriodType": "天任务",
        "jobVOS": [
            {
                "batchTask": {
                    "id": 805,
                    "gmtCreate": 1535623860000,
                    "gmtModified": 1536739910000,
                    "isDeleted": 0,
                    "tenantId": 129,
                    "projectId": 77,
                    "name": "job_dws_alg_trd_pay_cust_d",
                    "taskType": 0,
                    "computeType": 1,
                    "engineType": 1,
                    "sqlText": "CREATE TABLE if NOT EXISTS dws_alg_trd_pay_cust_1d(\nstatDate            STRING      COMMENT '统计日期'\n,tenantId           STRING      COMMENT '租户ID'\n,shopId             STRING      COMMENT '门店ID'\n,customerId         STRING      COMMENT '顾客ID'\n,max_payment_time   STRING      COMMENT '最近一天最晚支付时间'\n,inter_days_1d      BIGINT      COMMENT '最近一天顾客最后一次发生支付行为距离当前日期多少天'\n,pay_quantity_1d    BIGINT      COMMENT '最近一天顾客支付行为次数'\n,pay_amount_1d      DOUBLE      COMMENT '最近一天顾客支付总金额'\n,tot_days_1d        BIGINT      COMMENT '最近一天顾客出现支付行为的天数之和'\n) COMMENT '最近1天算法信息指标'\nPARTITIONED BY (ds STRING ) STORED AS orc LIFECYCLE 365;\n\nset spark.sql.shuffle.partitions=1;\nset hive.exec.dynamic.partition.mode=nonstrict;\n\n\nINSERT OVERWRITE TABLE dws_alg_trd_pay_cust_1d PARTITION(ds)\nSELECT\n  date_add(concat(substr(t.par, 0, 4), '-', substr(t.par, 5, 2), '-', substr(t.par, 7, 2)),1)        AS statDate,\n  t.tenantId              AS tenantId,\n  t.shopId                AS shopId,\n  t.customerId            AS customerId,\n  t.paymentTime           AS max_payment_time,\n  datediff(date_add(concat(substr(t.par, 0, 4), '-', substr(t.par, 5, 2), '-', substr(t.par, 7, 2)),1),\n           t.paymentTime) AS inter_days_1d,\n  t.pay_quantity_1d       AS pay_quantity_1d,\n  t.pay_amount_1d         AS pay_amount_1d,\n  t.tot_days_1d           AS tot_days_1d,\n  t.par                   AS ds\nFROM (\n       SELECT\n         tenantId,\n         min(shopId)                                                              AS shopId,\n         customerId,\n         max(paymentTime)                                                         AS paymentTime,\n         COUNT(1)                                                                 AS pay_quantity_1d,\n         sum(custActualPay)                                                       AS pay_amount_1d,\n         size(collect_set(substr(regexp_replace(paymentTime, '-', ''), 1, 8)))    AS tot_days_1d,\n         date_format(paymentTime, 'yyyyMMdd')                                     AS par\n       FROM dwd_ord_trd_d\n       WHERE ds = '${bdp.system.bizdate}' AND tradePayStatus = 3 AND isnotnull(customerId)\n       GROUP BY tenantId, customerId, date_format(paymentTime, 'yyyyMMdd')) t;\n",
                    "taskParams": "##Number of CPU cores need driver program is running\n##driver.cores=1\n\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\n##driver.maxResultSize=1g\n\n##Driver number memory used by a process\n##driver.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##executor.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##logConf=spark.logConf",
                    "scheduleConf": "{\"selfReliance\":0,\"min\":\"0\",\"hour\":\"4\",\"periodType\":\"2\",\"beginDate\":\"2001-01-02\",\"endDate\":\"2121-01-01\"}",
                    "periodType": null,
                    "scheduleStatus": 2,
                    "submitStatus": 1,
                    "modifyUserId": 90,
                    "createUserId": 90,
                    "ownerUserId": 90,
                    "version": 106,
                    "nodePid": 1146,
                    "taskDesc": "",
                    "mainClass": "",
                    "exeArgs": "",
                    "flowId": 0,
                    "createUser": {
                        "id": 90,
                        "gmtCreate": 1535092930000,
                        "gmtModified": 1535092930000,
                        "isDeleted": 0,
                        "userName": "yizhou@dtstack.com",
                        "phoneNumber": null,
                        "dtuicUserId": 6999,
                        "email": "",
                        "status": 0,
                        "defaultProjectId": null
                    },
                    "modifyUser": null,
                    "ownerUser": null,
                    "taskPeriodId": 2,
                    "taskPeriodType": "天任务",
                    "nodePName": null,
                    "readWriteLockVO": null,
                    "userId": null,
                    "lockVersion": null,
                    "taskVariables": null,
                    "forceUpdate": false,
                    "subNodes": null,
                    "relatedTasks": null,
                    "createModel": 0,
                    "operateModel": 0,
                    "pythonVersion": 0,
                    "learningType": 0,
                    "input": null,
                    "output": null,
                    "options": null,
                    "flowName": null,
                    "syncModel": 0,
                    "increColumn": null,
                    "taskVOS": null,
                    "subTaskVOS": null,
                    "resourceList": null,
                    "refResourceList": null,
                    "taskVersions": null,
                    "cron": "0 0 4 * * ?"
                },
                "id": 278692,
                "gmtCreate": 1547647311000,
                "gmtModified": 1547647311000,
                "isDeleted": 0,
                "tenantId": null,
                "projectId": null,
                "jobId": "79d7aea1",
                "jobKey": "cronTrigger_805_20190117040000",
                "jobName": "cronJob_job_dws_alg_trd_pay_cust_d_20190117040000",
                "status": 18,
                "taskId": 805,
                "createUserId": 90,
                "type": 0,
                "businessDate": "2019-01-16 ",
                "cycTime": "2019-01-17 04:00:00",
                "execStartTime": null,
                "execEndTime": null,
                "execTime": null,
                "execStartDate": null,
                "execEndDate": null,
                "taskPeriodId": 2,
                "taskPeriodType": "天任务",
                "jobVOS": null,
                "batchEngineJob": {
                    "id": 297190,
                    "gmtCreate": 1547647311000,
                    "gmtModified": 1547668893000,
                    "isDeleted": 0,
                    "status": 18,
                    "jobId": "79d7aea1",
                    "engineJobId": null,
                    "logInfo": "{ \"msg_info\": \"当前任务处于冻结状态\"}",
                    "engineLog": "",
                    "execStartTime": null,
                    "execEndTime": null,
                    "execTime": 0
                },
                "subNodes": null,
                "flowJobId": "0",
                "relatedJobs": null,
                "isDirty": 0
            },
            {
                "batchTask": {
                    "id": 860,
                    "gmtCreate": 1536145160000,
                    "gmtModified": 1537430157000,
                    "isDeleted": 0,
                    "tenantId": 129,
                    "projectId": 77,
                    "name": "job_dws_coupon_trd_1d",
                    "taskType": 0,
                    "computeType": 1,
                    "engineType": 1,
                    "sqlText": "CREATE TABLE if NOT EXISTS dws_coupon_trd_1d(\nstatDate            STRING    COMMENT '交易日期'\n,tenantId           STRING    COMMENT '租户ID'\n,service_order_id   STRING    COMMENT '发券活动ID'\n,gmv                DOUBLE    COMMENT '当日总营业额'\n,coupon_gmv         DOUBLE    COMMENT '当日核销产生营业额'\n,tc                 BIGINT    COMMENT '当日交易笔数'\n,pct                DOUBLE    COMMENT '当日客单价'\n,trade_user_count   DOUBLE    COMMENT '当日交易人数'\n,coupon_used        BIGINT    COMMENT '当日核销优惠券数量'\n,total_coupon_count STRING    COMMENT '当日优惠券总面额'\n,order_gmv          DOUBLE    COMMENT '当日订单总额'\n) COMMENT '发券后运营指标统计'\nPARTITIONED BY (ds STRING) STORED AS orc LIFECYCLE 1000;\n\n\nset spark.sql.shuffle.partitions=1;\nset hive.exec.dynamic.partition.mode=nonstrict;\n\n\nINSERT OVERWRITE TABLE dws_coupon_trd_1d PARTITION(ds)\nSELECT\n    total.paymentTime                                                      AS statDate, \n    total.tenantId,\n    total.service_order_id                                                 AS service_order_id,\n    sum(total.shopActualAmount)                                            AS gmv,\n    coalesce(sum(CASE\n               WHEN total.coupon_instance_id IS NOT NULL\n                 THEN total.shopActualAmount\n               END),0)                                                     AS coupon_gmv,\n    count(total.orderId)                                                   AS tc,\n    avg(total.shopActualAmount)                                            AS pct,\n    count(total.orderId)                                                   AS trade_user_count,\n    coalesce(sum(CASE\n               WHEN total.coupon_instance_id IS NOT NULL\n                 THEN 1\n               END),0)                                                     AS coupon_used,\n    coalesce(sum(CASE\n               WHEN total.coupon_instance_id IS NOT NULL\n                 THEN total.deno_value\n               END),0)                                                     AS total_coupon_count,\n    sum(tradeAmount)                                                       AS tradeAmount,\n    total.paymentTime                                                      AS ds\nFROM (\nSELECT \n    d.orderId                                AS orderId,\n    d.tenantId,\n    d.shopId,\n    d.customerId,\n    date_format(d.paymentTime,'yyyyMMdd')    AS paymentTime,\n    d.shopActualAmount,\n    d.tradeAmount,\n    d.custActualPay,\n    d.custShouldPay,\n    d.dishTotalAmount,\n    d.extraAmount,\n    d.privilegeAmount,\n    ins.id                                   AS coupon_instance_id,\n    ins.service_order_id                     AS service_order_id,\n    ins.couponType,\n    ins.deno_value,\n    ins.full_value\nFROM \n(SELECT \n    orderId,\n    tenantId,\n    shopId,\n    customerId,\n    paymentTime,\n    tradeNo,\n    shopActualAmount,\n    tradeAmount,\n    custActualPay,\n    custShouldPay,\n    dishTotalAmount,\n    extraAmount,\n    privilegeAmount\n FROM dwd_ord_trd_d WHERE ds = '${bdp.system.bizdate}' and tradeStatus=4) d \nLEFT OUTER JOIN  \n(SELECT \n    id,\n    tenantId,\n    shopId,\n    service_order_id,\n    customerId,\n    checkTime,\n    couponType,\n    send_date,\n    tradeNo,\n    deno_value,\n    full_value\n FROM dwd_coupon_instance_d WHERE ds = '${bdp.system.bizdate}' AND isnotnull(tradeNo) AND couponStatus = 2) ins \n ON d.tenantId = ins.tenantId AND d.tradeNo=ins.tradeNo) total  -- 去掉shopId的关联，因为instance的shopId不准确\nGROUP BY total.tenantId, total.service_order_id, total.paymentTime;\n\n",
                    "taskParams": "##Number of CPU cores need driver program is running\n##driver.cores=1\n\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\n##driver.maxResultSize=1g\n\n##Driver number memory used by a process\n##driver.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##executor.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##logConf=spark.logConf",
                    "scheduleConf": "{\"selfReliance\":0,\"min\":\"0\",\"hour\":\"4\",\"periodType\":\"2\",\"beginDate\":\"2001-01-01\",\"endDate\":\"2121-01-01\"}",
                    "periodType": null,
                    "scheduleStatus": 2,
                    "submitStatus": 1,
                    "modifyUserId": 90,
                    "createUserId": 90,
                    "ownerUserId": 92,
                    "version": 58,
                    "nodePid": 1189,
                    "taskDesc": "",
                    "mainClass": "",
                    "exeArgs": "",
                    "flowId": 0,
                    "createUser": {
                        "id": 90,
                        "gmtCreate": 1535092930000,
                        "gmtModified": 1535092930000,
                        "isDeleted": 0,
                        "userName": "yizhou@dtstack.com",
                        "phoneNumber": null,
                        "dtuicUserId": 6999,
                        "email": "",
                        "status": 0,
                        "defaultProjectId": null
                    },
                    "modifyUser": null,
                    "ownerUser": null,
                    "taskPeriodId": 2,
                    "taskPeriodType": "天任务",
                    "nodePName": null,
                    "readWriteLockVO": null,
                    "userId": null,
                    "lockVersion": null,
                    "taskVariables": null,
                    "forceUpdate": false,
                    "subNodes": null,
                    "relatedTasks": null,
                    "createModel": 0,
                    "operateModel": 0,
                    "pythonVersion": 0,
                    "learningType": 0,
                    "input": null,
                    "output": null,
                    "options": null,
                    "flowName": null,
                    "syncModel": 0,
                    "increColumn": null,
                    "taskVOS": null,
                    "subTaskVOS": null,
                    "resourceList": null,
                    "refResourceList": null,
                    "taskVersions": null,
                    "cron": "0 0 4 * * ?"
                },
                "id": 278693,
                "gmtCreate": 1547647311000,
                "gmtModified": 1547647311000,
                "isDeleted": 0,
                "tenantId": null,
                "projectId": null,
                "jobId": "e46e72a4",
                "jobKey": "cronTrigger_860_20190117040000",
                "jobName": "cronJob_job_dws_coupon_trd_1d_20190117040000",
                "status": 18,
                "taskId": 860,
                "createUserId": 90,
                "type": 0,
                "businessDate": "2019-01-16 ",
                "cycTime": "2019-01-17 04:00:00",
                "execStartTime": null,
                "execEndTime": null,
                "execTime": null,
                "execStartDate": null,
                "execEndDate": null,
                "taskPeriodId": 2,
                "taskPeriodType": "天任务",
                "jobVOS": null,
                "batchEngineJob": {
                    "id": 297191,
                    "gmtCreate": 1547647311000,
                    "gmtModified": 1547668893000,
                    "isDeleted": 0,
                    "status": 18,
                    "jobId": "e46e72a4",
                    "engineJobId": null,
                    "logInfo": "{ \"msg_info\": \"当前任务处于冻结状态\"}",
                    "engineLog": "",
                    "execStartTime": null,
                    "execEndTime": null,
                    "execTime": 0
                },
                "subNodes": null,
                "flowJobId": "0",
                "relatedJobs": null,
                "isDirty": 0
            },
            {
                "batchTask": {
                    "id": 959,
                    "gmtCreate": 1536571318000,
                    "gmtModified": 1537154763000,
                    "isDeleted": 0,
                    "tenantId": 129,
                    "projectId": 77,
                    "name": "job_dws_ope_trd_pay_tenant_1d",
                    "taskType": 0,
                    "computeType": 1,
                    "engineType": 1,
                    "sqlText": "CREATE TABLE if NOT EXISTS dws_ope_trd_pay_tenant_1d(\nstatDate                        STRING      COMMENT '统计日期' \n,tenantId                       STRING      COMMENT '租户ID'\n,guest_cnt_1d                   BIGINT      COMMENT '日散客数'\n,act_member_gmv_1d              DOUBLE      COMMENT '日激活会员交易额'\n,unact_member_gmv_1d            DOUBLE      COMMENT '日未激活会员交易额'\n,guest_gmv_1d                   DOUBLE      COMMENT '日散客交易额'\n,act_member_tc_1d               BIGINT      COMMENT '日激活会员交易次数'\n,unact_member_tc_1d             BIGINT      COMMENT '日未激活会员交易次数'\n) COMMENT '最近一天租户粒度销售汇总表'\nPARTITIONED BY (ds STRING) STORED AS orc LIFECYCLE 365;\n\nset spark.sql.shuffle.partitions=1;\nset hive.exec.dynamic.partition.mode=nonstrict;\n\n-- comment (0-散客，1-未激活，2-激活)\nINSERT OVERWRITE TABLE dws_ope_trd_pay_tenant_1d PARTITION (ds)\nSELECT\n  '${bdp.system.bizdate}'                          AS statDate,\n  total.tenantId,\n  coalesce(sum(CASE total.isIndividual\n               WHEN 0\n                 THEN 1 END), 0)                   AS guest_cnt_1d,\n  coalesce(sum(CASE total.isIndividual\n               WHEN 2\n                 THEN total.custActualPay END), 0) AS act_member_gmv_1d,\n  coalesce(sum(CASE total.isIndividual\n               WHEN 1\n                 THEN total.custActualPay END), 0) AS unact_member_gmv_1d,\n  coalesce(sum(CASE total.isIndividual\n               WHEN 0\n                 THEN total.custActualPay END), 0) AS guest_gmv_1d,\n  coalesce(sum(CASE total.isIndividual\n               WHEN 2\n                 THEN 1 END), 0)                   AS act_member_tc,\n  coalesce(sum(CASE total.isIndividual\n               WHEN 1\n                 THEN 1 END), 0)                   AS unact_member_tc,\n  total.paymentTime                                AS ds\nFROM\n  (SELECT\n     o.tenantId,\n     if(isnull(o.customerId), 0,\n        if(isnull(o.memberId), 1, 2))       AS isIndividual,    -- (0-散客，1-未激活，2-激活)\n     o.shopActualAmount                     AS shopActualAmount,\n     date_format(o.paymentTime, 'yyyyMMdd') AS paymentTime,\n     o.custActualPay\n   FROM dwd_ord_trd_d o\n   WHERE o.ds = '${bdp.system.bizdate}' AND o.tradeStatus = 4) total\nGROUP BY total.tenantId, total.paymentTime;\n",
                    "taskParams": "##Number of CPU cores need driver program is running\n##driver.cores=1\n\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\n##driver.maxResultSize=1g\n\n##Driver number memory used by a process\n##driver.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##executor.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##logConf=spark.logConf",
                    "scheduleConf": "{\"selfReliance\":0,\"min\":\"0\",\"hour\":\"4\",\"periodType\":\"2\",\"beginDate\":\"2001-01-01\",\"endDate\":\"2121-01-01\"}",
                    "periodType": null,
                    "scheduleStatus": 2,
                    "submitStatus": 1,
                    "modifyUserId": 90,
                    "createUserId": 90,
                    "ownerUserId": 90,
                    "version": 20,
                    "nodePid": 1168,
                    "taskDesc": "",
                    "mainClass": "",
                    "exeArgs": "",
                    "flowId": 0,
                    "createUser": {
                        "id": 90,
                        "gmtCreate": 1535092930000,
                        "gmtModified": 1535092930000,
                        "isDeleted": 0,
                        "userName": "yizhou@dtstack.com",
                        "phoneNumber": null,
                        "dtuicUserId": 6999,
                        "email": "",
                        "status": 0,
                        "defaultProjectId": null
                    },
                    "modifyUser": null,
                    "ownerUser": null,
                    "taskPeriodId": 2,
                    "taskPeriodType": "天任务",
                    "nodePName": null,
                    "readWriteLockVO": null,
                    "userId": null,
                    "lockVersion": null,
                    "taskVariables": null,
                    "forceUpdate": false,
                    "subNodes": null,
                    "relatedTasks": null,
                    "createModel": 0,
                    "operateModel": 0,
                    "pythonVersion": 0,
                    "learningType": 0,
                    "input": null,
                    "output": null,
                    "options": null,
                    "flowName": null,
                    "syncModel": 0,
                    "increColumn": null,
                    "taskVOS": null,
                    "subTaskVOS": null,
                    "resourceList": null,
                    "refResourceList": null,
                    "taskVersions": null,
                    "cron": "0 0 4 * * ?"
                },
                "id": 278695,
                "gmtCreate": 1547647311000,
                "gmtModified": 1547647311000,
                "isDeleted": 0,
                "tenantId": null,
                "projectId": null,
                "jobId": "4402628a",
                "jobKey": "cronTrigger_959_20190117040000",
                "jobName": "cronJob_job_dws_ope_trd_pay_tenant_1d_20190117040000",
                "status": 18,
                "taskId": 959,
                "createUserId": 90,
                "type": 0,
                "businessDate": "2019-01-16 ",
                "cycTime": "2019-01-17 04:00:00",
                "execStartTime": null,
                "execEndTime": null,
                "execTime": null,
                "execStartDate": null,
                "execEndDate": null,
                "taskPeriodId": 2,
                "taskPeriodType": "天任务",
                "jobVOS": null,
                "batchEngineJob": {
                    "id": 297193,
                    "gmtCreate": 1547647311000,
                    "gmtModified": 1547668893000,
                    "isDeleted": 0,
                    "status": 18,
                    "jobId": "4402628a",
                    "engineJobId": null,
                    "logInfo": "{ \"msg_info\": \"当前任务处于冻结状态\"}",
                    "engineLog": "",
                    "execStartTime": null,
                    "execEndTime": null,
                    "execTime": 0
                },
                "subNodes": null,
                "flowJobId": "0",
                "relatedJobs": null,
                "isDirty": 0
            },
            {
                "batchTask": {
                    "id": 960,
                    "gmtCreate": 1536572510000,
                    "gmtModified": 1537340948000,
                    "isDeleted": 0,
                    "tenantId": 129,
                    "projectId": 77,
                    "name": "job_dws_ope_ma_trade_trend_1d",
                    "taskType": 0,
                    "computeType": 1,
                    "engineType": 1,
                    "sqlText": "CREATE TABLE if NOT EXISTS dws_ope_ma_trade_trend_1d(\nstatDate                     STRING COMMENT '统计日期'\n,day_id                      STRING COMMENT '天ID'\n,week_id                     STRING COMMENT '周ID'\n,month_id                    STRING COMMENT '月ID'\n,quarter_id                  STRING COMMENT '季度ID'\n,year_id                     STRING COMMENT '年度ID'\n,tenantId                    STRING COMMENT '租户ID'\n,shopId                      STRING COMMENT '门店ID'\n,gmv                         DOUBLE COMMENT '交易总额'\n,tc                          BIGINT COMMENT '交易笔数'\n,pct                         DOUBLE COMMENT '客单价'\n) COMMENT '门店交易日趋势1天'\nPARTITIONED BY (ds STRING ) STORED AS orc LIFECYCLE 1000;\n\n\nset spark.sql.shuffle.partitions=1;\nset hive.exec.dynamic.partition.mode=nonstrict;\n\n\nINSERT OVERWRITE TABLE dws_ope_ma_trade_trend_1d PARTITION (ds)\nSELECT\n'${bdp.system.bizdate}' AS statDate,\ntime.day_id             AS day_id,\ntime.week_id            AS week_id,\ntime.month_id           AS month_id,\ntime.quarter_id         AS quarter_id,\ntime.year_id            AS year_id,\nres.tenantId,\nres.shopId,\nres.gmv,\nres.tc,\nres.pct,\nres.paymentTime         AS ds \nFROM \n(SELECT\n  tenantId,\n  shopId,\n  date_format(paymentTime,'yyyyMMdd') AS paymentTime,\n  sum(shopActualAmount)               AS gmv, -- 交易额 \n  count(DISTINCT orderId)             AS tc,  -- 交易笔数 \n  avg(shopActualAmount)               AS pct  -- 客单价 \nFROM dwd_ord_trd_d\nWHERE\n  tradeStatus = 4 AND ds = '${bdp.system.bizdate}' \nGROUP BY tenantId, shopId, date_format(paymentTime,'yyyyMMdd')\nUNION \nSELECT\n  tenantId,\n  '0'                                 AS shopId,\n  date_format(paymentTime,'yyyyMMdd') AS paymentTime,\n  sum(shopActualAmount)               AS gmv, -- 交易额 \n  count(DISTINCT orderId)             AS tc,  -- 交易笔数 \n  avg(shopActualAmount)               AS pct  -- 客单价 \nFROM dwd_ord_trd_d\nWHERE\n  tradeStatus = 4 AND ds = '${bdp.system.bizdate}'\nGROUP BY tenantId, date_format(paymentTime,'yyyyMMdd')) res \n LEFT OUTER JOIN dim_unit_day time ON res.paymentTime = time.day_id AND time.ds = '${bdp.system.bizdate}';\n",
                    "taskParams": "##Number of CPU cores need driver program is running\n##driver.cores=1\n\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\n##driver.maxResultSize=1g\n\n##Driver number memory used by a process\n##driver.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##executor.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##logConf=spark.logConf",
                    "scheduleConf": "{\"selfReliance\":0,\"min\":\"0\",\"hour\":\"4\",\"periodType\":\"2\",\"beginDate\":\"2001-01-01\",\"endDate\":\"2121-01-01\"}",
                    "periodType": null,
                    "scheduleStatus": 2,
                    "submitStatus": 1,
                    "modifyUserId": 90,
                    "createUserId": 90,
                    "ownerUserId": 90,
                    "version": 19,
                    "nodePid": 1167,
                    "taskDesc": "",
                    "mainClass": "",
                    "exeArgs": "",
                    "flowId": 0,
                    "createUser": {
                        "id": 90,
                        "gmtCreate": 1535092930000,
                        "gmtModified": 1535092930000,
                        "isDeleted": 0,
                        "userName": "yizhou@dtstack.com",
                        "phoneNumber": null,
                        "dtuicUserId": 6999,
                        "email": "",
                        "status": 0,
                        "defaultProjectId": null
                    },
                    "modifyUser": null,
                    "ownerUser": null,
                    "taskPeriodId": 2,
                    "taskPeriodType": "天任务",
                    "nodePName": null,
                    "readWriteLockVO": null,
                    "userId": null,
                    "lockVersion": null,
                    "taskVariables": null,
                    "forceUpdate": false,
                    "subNodes": null,
                    "relatedTasks": null,
                    "createModel": 0,
                    "operateModel": 0,
                    "pythonVersion": 0,
                    "learningType": 0,
                    "input": null,
                    "output": null,
                    "options": null,
                    "flowName": null,
                    "syncModel": 0,
                    "increColumn": null,
                    "taskVOS": null,
                    "subTaskVOS": null,
                    "resourceList": null,
                    "refResourceList": null,
                    "taskVersions": null,
                    "cron": "0 0 4 * * ?"
                },
                "id": 278696,
                "gmtCreate": 1547647311000,
                "gmtModified": 1547647311000,
                "isDeleted": 0,
                "tenantId": null,
                "projectId": null,
                "jobId": "06d5d43c",
                "jobKey": "cronTrigger_960_20190117040000",
                "jobName": "cronJob_job_dws_ope_ma_trade_trend_1d_20190117040000",
                "status": 18,
                "taskId": 960,
                "createUserId": 90,
                "type": 0,
                "businessDate": "2019-01-16 ",
                "cycTime": "2019-01-17 04:00:00",
                "execStartTime": null,
                "execEndTime": null,
                "execTime": null,
                "execStartDate": null,
                "execEndDate": null,
                "taskPeriodId": 2,
                "taskPeriodType": "天任务",
                "jobVOS": null,
                "batchEngineJob": {
                    "id": 297194,
                    "gmtCreate": 1547647311000,
                    "gmtModified": 1547668893000,
                    "isDeleted": 0,
                    "status": 18,
                    "jobId": "06d5d43c",
                    "engineJobId": null,
                    "logInfo": "{ \"msg_info\": \"当前任务处于冻结状态\"}",
                    "engineLog": "",
                    "execStartTime": null,
                    "execEndTime": null,
                    "execTime": 0
                },
                "subNodes": null,
                "flowJobId": "0",
                "relatedJobs": null,
                "isDirty": 0
            },
            {
                "batchTask": {
                    "id": 961,
                    "gmtCreate": 1536573388000,
                    "gmtModified": 1537349671000,
                    "isDeleted": 0,
                    "tenantId": 129,
                    "projectId": 77,
                    "name": "job_dws_ope_ma_order_channel_trend_1d",
                    "taskType": 0,
                    "computeType": 1,
                    "engineType": 1,
                    "sqlText": "CREATE TABLE if NOT EXISTS  dws_ope_ma_order_channel_trend_1d(\nstatDate             STRING      COMMENT '统计日期'\n,day_id              STRING      COMMENT '天ID'\n,week_id             STRING      COMMENT '周ID'\n,month_id            STRING      COMMENT '月ID'\n,quarter_id          STRING      COMMENT '季度ID'\n,year_id             STRING      COMMENT '年度ID'\n,tenantId            STRING      COMMENT '租户ID'\n,shopId              STRING      COMMENT '门店ID'\n,channel_dim         BIGINT      COMMENT '订单渠道'\n,gmv                 DOUBLE      COMMENT '交易总额'\n,privilege_gmv       DOUBLE      COMMENT '优惠总额'\n,order_gmv           DOUBLE      COMMENT '订单总额'\n,roi                 DOUBLE      COMMENT 'roi'\n) COMMENT '门店订单渠道趋势分析1天'\nPARTITIONED BY (ds STRING ) STORED AS orc LIFECYCLE 365;\n\n\nset spark.sql.shuffle.partitions=1;\nset hive.exec.dynamic.partition.mode=nonstrict;\n\n\nINSERT OVERWRITE TABLE dws_ope_ma_order_channel_trend_1d PARTITION (ds)\nSELECT\n  '${bdp.system.bizdate}' AS statDate,\n  time.day_id             AS day_id,\n  time.week_id            AS week_id,\n  time.month_id           AS month_id,\n  time.quarter_id         AS quarter_id,\n  time.year_id            AS year_id,\n  res.tenantId            AS tenantId,\n  res.shopId              AS shopId,\n  res.dimId               AS channel_dim,\n  res.gmv                 AS gmv,\n  res.privilege_gmv       AS privilege_gmv,\n  res.order_gmv           AS order_gmv,\n  res.roi                 AS roi,\n  res.paymentTime         AS ds\nFROM (\n       SELECT\n         d.tenantId                                                         AS tenantId,\n         d.shopId                                                           AS shopId,\n         d.paymentTime                                                      AS paymentTime,\n         d.dimId                                                            AS dimId,\n         sum(d.shopActualAmount)                                            AS gmv,\n         sum(d.custActualPay - d.shopActualAmount)                          AS privilege_gmv,\n         sum(d.tradeAmount)                                                 AS order_gmv, -- 订单原价\n         coalesce(sum(d.shopActualAmount) / sum(d.tradeAmount - d.shopActualAmount),0)  AS roi        -- ROI：商家实收/(订单原价-商家实收)\n       FROM\n         (\n           SELECT\n             tenantId,\n             shopId,\n             date_format(paymentTime,'yyyyMMdd') AS paymentTime,\n             shopActualAmount,\n             custActualPay,\n             tradeAmount,\n             (CASE\n              WHEN sourceChild IN (1, 2, 3, 141, 142)\n                THEN 31\n              WHEN sourceChild IN (31, 32, 33)\n                THEN 32\n              WHEN sourceChild = 161\n                THEN 33\n              WHEN sourceChild = 181\n                THEN 34\n              ELSE 35\n              END) AS dimId\n           FROM dwd_ord_trd_d\n           WHERE ds = '${bdp.system.bizdate}' AND tradeStatus = 4) d\n       GROUP BY d.tenantId, d.shopId, d.dimId, d.paymentTime\n       UNION\n       SELECT\n         dd.tenantId,\n         '0'                                                                   AS shopId,\n         dd.paymentTime                                                        AS paymentTime,\n         dd.dimId,\n         sum(dd.shopActualAmount)                                              AS gmv,\n         sum(dd.custActualPay - dd.shopActualAmount)                           AS privilege_gmv,\n         sum(dd.tradeAmount)                                                   AS order_gmv, -- 订单原价\n         coalesce(sum(dd.shopActualAmount) / sum(dd.tradeAmount - dd.shopActualAmount),0)  AS roi        -- ROI：商家实收/(订单原价-商家实收)\n       FROM\n         (\n           SELECT\n             tenantId,\n             date_format(paymentTime,'yyyyMMdd') AS paymentTime,\n             shopActualAmount,\n             custActualPay,\n             tradeAmount,\n             (CASE\n              WHEN sourceChild IN (1, 2, 3, 141, 142)\n                THEN 31\n              WHEN sourceChild IN (31, 32, 33)\n                THEN 32\n              WHEN sourceChild = 161\n                THEN 33\n              WHEN sourceChild = 181\n                THEN 34\n              ELSE 35\n              END) AS dimId\n           FROM dwd_ord_trd_d\n           WHERE ds = '${bdp.system.bizdate}' AND tradeStatus = 4) dd\n       GROUP BY dd.tenantId, dd.dimId, dd.paymentTime) res\n    LEFT OUTER JOIN dim_unit_day time ON res.paymentTime = time.day_id AND time.ds = '${bdp.system.bizdate}';\n\n\n",
                    "taskParams": "##Number of CPU cores need driver program is running\n##driver.cores=1\n\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\n##driver.maxResultSize=1g\n\n##Driver number memory used by a process\n##driver.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##executor.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##logConf=spark.logConf",
                    "scheduleConf": "{\"selfReliance\":0,\"min\":\"0\",\"hour\":\"4\",\"periodType\":\"2\",\"beginDate\":\"2001-01-01\",\"endDate\":\"2121-01-01\"}",
                    "periodType": null,
                    "scheduleStatus": 2,
                    "submitStatus": 1,
                    "modifyUserId": 90,
                    "createUserId": 90,
                    "ownerUserId": 90,
                    "version": 18,
                    "nodePid": 1167,
                    "taskDesc": "",
                    "mainClass": "",
                    "exeArgs": "",
                    "flowId": 0,
                    "createUser": {
                        "id": 90,
                        "gmtCreate": 1535092930000,
                        "gmtModified": 1535092930000,
                        "isDeleted": 0,
                        "userName": "yizhou@dtstack.com",
                        "phoneNumber": null,
                        "dtuicUserId": 6999,
                        "email": "",
                        "status": 0,
                        "defaultProjectId": null
                    },
                    "modifyUser": null,
                    "ownerUser": null,
                    "taskPeriodId": 2,
                    "taskPeriodType": "天任务",
                    "nodePName": null,
                    "readWriteLockVO": null,
                    "userId": null,
                    "lockVersion": null,
                    "taskVariables": null,
                    "forceUpdate": false,
                    "subNodes": null,
                    "relatedTasks": null,
                    "createModel": 0,
                    "operateModel": 0,
                    "pythonVersion": 0,
                    "learningType": 0,
                    "input": null,
                    "output": null,
                    "options": null,
                    "flowName": null,
                    "syncModel": 0,
                    "increColumn": null,
                    "taskVOS": null,
                    "subTaskVOS": null,
                    "resourceList": null,
                    "refResourceList": null,
                    "taskVersions": null,
                    "cron": "0 0 4 * * ?"
                },
                "id": 278697,
                "gmtCreate": 1547647311000,
                "gmtModified": 1547647311000,
                "isDeleted": 0,
                "tenantId": null,
                "projectId": null,
                "jobId": "1c4df343",
                "jobKey": "cronTrigger_961_20190117040000",
                "jobName": "cronJob_job_dws_ope_ma_order_channel_trend_1d_20190117040000",
                "status": 18,
                "taskId": 961,
                "createUserId": 90,
                "type": 0,
                "businessDate": "2019-01-16 ",
                "cycTime": "2019-01-17 04:00:00",
                "execStartTime": null,
                "execEndTime": null,
                "execTime": null,
                "execStartDate": null,
                "execEndDate": null,
                "taskPeriodId": 2,
                "taskPeriodType": "天任务",
                "jobVOS": null,
                "batchEngineJob": {
                    "id": 297195,
                    "gmtCreate": 1547647311000,
                    "gmtModified": 1547668893000,
                    "isDeleted": 0,
                    "status": 18,
                    "jobId": "1c4df343",
                    "engineJobId": null,
                    "logInfo": "{ \"msg_info\": \"当前任务处于冻结状态\"}",
                    "engineLog": "",
                    "execStartTime": null,
                    "execEndTime": null,
                    "execTime": 0
                },
                "subNodes": null,
                "flowJobId": "0",
                "relatedJobs": null,
                "isDirty": 0
            },
            {
                "batchTask": {
                    "id": 967,
                    "gmtCreate": 1536588523000,
                    "gmtModified": 1536653122000,
                    "isDeleted": 0,
                    "tenantId": 129,
                    "projectId": 77,
                    "name": "job_dws_ope_ma_overview_1d",
                    "taskType": 0,
                    "computeType": 1,
                    "engineType": 1,
                    "sqlText": "CREATE TABLE if NOT EXISTS dws_ope_ma_overview_1d(\nstat_date            STRING      COMMENT '统计日期'\n,tenantId            STRING      COMMENT '租户ID'\n,shopId              STRING      COMMENT '门店ID'\n,order_gmv           DOUBLE      COMMENT '当日订单总额'\n,gmv                 DOUBLE      COMMENT '当日交易总额'\n,privilege_gmv       DOUBLE      COMMENT '当日优惠总额'\n,tc                  INT         COMMENT '当日交易笔数'\n,gmt_created         STRING      COMMENT '创建时间'\n,gmt_modified        STRING      COMMENT '修改时间'\n) COMMENT '门店每日经营概况'\nPARTITIONED BY (ds STRING COMMENT '时间分区') STORED AS orc LIFECYCLE 365;\n\nset spark.sql.shuffle.partitions=1;\nset hive.exec.dynamic.partition.mode=nonstrict;\n\nINSERT OVERWRITE TABLE dws_ope_ma_overview_1d PARTITION (ds)\nSELECT\n  '${bdp.system.bizdate}' AS statDate,\n  res.tenantId,\n  res.shopId,\n  res.order_gmv,\n  res.gmv,\n  res.privilege_gmv,\n  res.tc,\n  '${bdp.system.cyctime}' AS gmt_created,\n  '${bdp.system.cyctime}' AS gmt_modified,\n  res.paymentTime         AS ds\nFROM (SELECT\n        tenantId,\n        shopId,\n        date_format(paymentTime, 'yyyyMMdd')  AS paymentTime,\n        sum(custActualPay)                    AS order_gmv,\n        sum(shopActualAmount)                 AS gmv,\n        sum(custActualPay - shopActualAmount) AS privilege_gmv,\n        count(DISTINCT orderId)               AS tc\n      FROM dwd_ord_trd_d\n      WHERE tradeStatus = 4 AND ds = '${bdp.system.bizdate}'\n      GROUP BY tenantId, shopId, date_format(paymentTime, 'yyyyMMdd')\n      UNION\n      SELECT\n        tenantId,\n        0,\n        date_format(paymentTime, 'yyyyMMdd')  AS paymentTime,\n        sum(custActualPay)                    AS order_gmv,\n        sum(shopActualAmount)                 AS gmv,\n        sum(custActualPay - shopActualAmount) AS privilege_gmv,\n        count(DISTINCT orderId)               AS tc\n      FROM dwd_ord_trd_d\n      WHERE tradeStatus = 4 AND ds = '${bdp.system.bizdate}'\n      GROUP BY tenantId, date_format(paymentTime, 'yyyyMMdd')) res;\n\n",
                    "taskParams": "##Number of CPU cores need driver program is running\n##driver.cores=1\n\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\n##driver.maxResultSize=1g\n\n##Driver number memory used by a process\n##driver.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##executor.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##logConf=spark.logConf",
                    "scheduleConf": "{\"selfReliance\":0,\"min\":\"0\",\"hour\":\"4\",\"periodType\":\"2\",\"beginDate\":\"2001-01-01\",\"endDate\":\"2121-01-01\"}",
                    "periodType": null,
                    "scheduleStatus": 2,
                    "submitStatus": 1,
                    "modifyUserId": 90,
                    "createUserId": 90,
                    "ownerUserId": 90,
                    "version": 9,
                    "nodePid": 1167,
                    "taskDesc": "",
                    "mainClass": "",
                    "exeArgs": "",
                    "flowId": 0,
                    "createUser": {
                        "id": 90,
                        "gmtCreate": 1535092930000,
                        "gmtModified": 1535092930000,
                        "isDeleted": 0,
                        "userName": "yizhou@dtstack.com",
                        "phoneNumber": null,
                        "dtuicUserId": 6999,
                        "email": "",
                        "status": 0,
                        "defaultProjectId": null
                    },
                    "modifyUser": null,
                    "ownerUser": null,
                    "taskPeriodId": 2,
                    "taskPeriodType": "天任务",
                    "nodePName": null,
                    "readWriteLockVO": null,
                    "userId": null,
                    "lockVersion": null,
                    "taskVariables": null,
                    "forceUpdate": false,
                    "subNodes": null,
                    "relatedTasks": null,
                    "createModel": 0,
                    "operateModel": 0,
                    "pythonVersion": 0,
                    "learningType": 0,
                    "input": null,
                    "output": null,
                    "options": null,
                    "flowName": null,
                    "syncModel": 0,
                    "increColumn": null,
                    "taskVOS": null,
                    "subTaskVOS": null,
                    "resourceList": null,
                    "refResourceList": null,
                    "taskVersions": null,
                    "cron": "0 0 4 * * ?"
                },
                "id": 278699,
                "gmtCreate": 1547647311000,
                "gmtModified": 1547647311000,
                "isDeleted": 0,
                "tenantId": null,
                "projectId": null,
                "jobId": "cd460c88",
                "jobKey": "cronTrigger_967_20190117040000",
                "jobName": "cronJob_job_dws_ope_ma_overview_1d_20190117040000",
                "status": 18,
                "taskId": 967,
                "createUserId": 90,
                "type": 0,
                "businessDate": "2019-01-16 ",
                "cycTime": "2019-01-17 04:00:00",
                "execStartTime": null,
                "execEndTime": null,
                "execTime": null,
                "execStartDate": null,
                "execEndDate": null,
                "taskPeriodId": 2,
                "taskPeriodType": "天任务",
                "jobVOS": null,
                "batchEngineJob": {
                    "id": 297197,
                    "gmtCreate": 1547647311000,
                    "gmtModified": 1547668893000,
                    "isDeleted": 0,
                    "status": 18,
                    "jobId": "cd460c88",
                    "engineJobId": null,
                    "logInfo": "{ \"msg_info\": \"当前任务处于冻结状态\"}",
                    "engineLog": "",
                    "execStartTime": null,
                    "execEndTime": null,
                    "execTime": 0
                },
                "subNodes": null,
                "flowJobId": "0",
                "relatedJobs": null,
                "isDirty": 0
            },
            {
                "batchTask": {
                    "id": 1050,
                    "gmtCreate": 1537324920000,
                    "gmtModified": 1537327827000,
                    "isDeleted": 0,
                    "tenantId": 129,
                    "projectId": 77,
                    "name": "dws_alg_flow_job",
                    "taskType": 10,
                    "computeType": 1,
                    "engineType": 1,
                    "sqlText": "[{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1051,\"parentId\":1193,\"name\":\"job_dws_alg_1d\",\"level\":null,\"type\":\"file\",\"taskType\":0,\"resourceType\":null,\"catalogueType\":\"TaskDevelop\",\"createUser\":\"yizhou@dtstack.com\",\"orderVal\":null,\"children\":null,\"readWriteLockVO\":{\"id\":1166,\"gmtCreate\":null,\"gmtModified\":1537327018992,\"isDeleted\":0,\"lockName\":\"1051_77_BATCH_TASK\",\"modifyUserId\":90,\"version\":1,\"projectId\":77,\"relationId\":1051,\"type\":\"BATCH_TASK\",\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"version\":0,\"operateModel\":1,\"pythonVersion\":0,\"learningType\":0,\"scriptType\":null,\"isSubTask\":0,\"scheduleStatus\":1,\"submitStatus\":0,\"catalogues\":null,\"tasks\":null,\"dependencyTasks\":null},\"x\":425,\"y\":0,\"value\":null,\"id\":1051},{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1052,\"gmtCreate\":1537327160000,\"gmtModified\":1537327160000,\"isDeleted\":0,\"tenantId\":129,\"projectId\":77,\"name\":\"job_dws_alg_3d\",\"taskType\":0,\"computeType\":1,\"engineType\":1,\"sqlText\":\"CREATE TABLE if NOT EXISTS dws_alg_trd_pay_cust_3d(\\nstatDate            STRING      COMMENT '统计日期'\\n,tenantId           STRING      COMMENT '租户ID'\\n,shopId             STRING      COMMENT '门店ID'\\n,customerId         STRING      COMMENT '顾客ID'\\n,inter_days_3d      BIGINT      COMMENT '最近3天顾客最后一次发生支付行为距离当前日期多少天'\\n,pay_quantity_3d    BIGINT      COMMENT '最近3天顾客支付行为次数'\\n,pay_amount_3d      DOUBLE      COMMENT '最近3天顾客支付总金额'\\n,tot_days_3d        BIGINT      COMMENT '最近3天顾客出现支付行为的天数之和'\\n) COMMENT '最近3天算法信息指标'\\nPARTITIONED BY (ds STRING ) STORED AS orc LIFECYCLE 365;\\n\\nset spark.sql.shuffle.partitions=1;\\n\\nINSERT OVERWRITE TABLE dws_alg_trd_pay_cust_3d PARTITION(ds = '${bdp.system.bizdate}')\\nSELECT\\n  date_add(concat(\\n               substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\\n               substr('${bdp.system.bizdate}', 7, 2)), 1) AS statDate,\\n  tenantId,\\n  min(shopId)                                             AS shopId,\\n  customerId,\\n  datediff(date_add(concat(substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\\n                           substr('${bdp.system.bizdate}', 7, 2)), 1),\\n           max(max_payment_time))                         AS inter_days_3d,\\n  sum(pay_quantity_1d)                                    AS pay_quantity_3d,\\n  sum(pay_amount_1d)                                      AS pay_amount_3d,\\n  sum(tot_days_1d)                                        AS tot_days_3d\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE ds BETWEEN '${key_3d}' AND '${bdp.system.bizdate}'\\nGROUP BY tenantId, customerId;\\n\",\"taskParams\":\"##Number of CPU cores need driver program is running\\n##driver.cores=1\\n\\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\\n##driver.maxResultSize=1g\\n\\n##Driver number memory used by a process\\n##driver.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##executor.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##logConf=spark.logConf\",\"scheduleConf\":\"{\\\"selfReliance\\\":0,\\\"min\\\":\\\"0\\\",\\\"hour\\\":\\\"4\\\",\\\"periodType\\\":\\\"2\\\",\\\"beginDate\\\":\\\"2001-01-01\\\",\\\"endDate\\\":\\\"2121-01-01\\\"}\",\"periodType\":null,\"scheduleStatus\":1,\"submitStatus\":0,\"modifyUserId\":90,\"createUserId\":90,\"ownerUserId\":90,\"version\":2,\"nodePid\":1193,\"taskDesc\":\"\",\"mainClass\":\"\",\"exeArgs\":\"\",\"flowId\":1050,\"createUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"modifyUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"ownerUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"taskPeriodId\":2,\"taskPeriodType\":\"天任务\",\"nodePName\":\"工作流\",\"readWriteLockVO\":{\"id\":0,\"gmtCreate\":null,\"gmtModified\":1537327159000,\"isDeleted\":0,\"lockName\":null,\"modifyUserId\":null,\"version\":1,\"projectId\":null,\"relationId\":null,\"type\":null,\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"userId\":90,\"lockVersion\":null,\"taskVariables\":[{\"paramName\":\"bdp.system.bizdate\",\"paramCommand\":\"yyyyMMdd-1\",\"type\":0},{\"paramName\":\"key_3d\",\"paramCommand\":\"$[yyyyMMdd-3]\",\"type\":1}],\"forceUpdate\":false,\"subNodes\":null,\"createModel\":0,\"operateModel\":0,\"pythonVersion\":0,\"learningType\":0,\"input\":null,\"output\":null,\"options\":null,\"flowName\":\"flow_job\",\"taskVOS\":null,\"subTaskVOS\":null,\"resourceList\":[],\"taskVersions\":[],\"cron\":\"0 0 0 * * ?\",\"merged\":false,\"cursorPosition\":{\"lineNumber\":32,\"column\":1}},\"x\":0,\"y\":80,\"value\":null,\"id\":1052},{\"vertex\":false,\"edge\":true,\"x\":0,\"y\":0,\"value\":\"\",\"id\":\"1054\",\"source\":{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1051,\"parentId\":1193,\"name\":\"job_dws_alg_1d\",\"level\":null,\"type\":\"file\",\"taskType\":0,\"resourceType\":null,\"catalogueType\":\"TaskDevelop\",\"createUser\":\"yizhou@dtstack.com\",\"orderVal\":null,\"children\":null,\"readWriteLockVO\":{\"id\":1166,\"gmtCreate\":null,\"gmtModified\":1537327018992,\"isDeleted\":0,\"lockName\":\"1051_77_BATCH_TASK\",\"modifyUserId\":90,\"version\":1,\"projectId\":77,\"relationId\":1051,\"type\":\"BATCH_TASK\",\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"version\":0,\"operateModel\":1,\"pythonVersion\":0,\"learningType\":0,\"scriptType\":null,\"isSubTask\":0,\"scheduleStatus\":1,\"submitStatus\":0,\"catalogues\":null,\"tasks\":null,\"dependencyTasks\":null},\"x\":425,\"y\":0,\"value\":null,\"id\":1051},\"target\":{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1052,\"gmtCreate\":1537327160000,\"gmtModified\":1537327160000,\"isDeleted\":0,\"tenantId\":129,\"projectId\":77,\"name\":\"job_dws_alg_3d\",\"taskType\":0,\"computeType\":1,\"engineType\":1,\"sqlText\":\"CREATE TABLE if NOT EXISTS dws_alg_trd_pay_cust_3d(\\nstatDate            STRING      COMMENT '统计日期'\\n,tenantId           STRING      COMMENT '租户ID'\\n,shopId             STRING      COMMENT '门店ID'\\n,customerId         STRING      COMMENT '顾客ID'\\n,inter_days_3d      BIGINT      COMMENT '最近3天顾客最后一次发生支付行为距离当前日期多少天'\\n,pay_quantity_3d    BIGINT      COMMENT '最近3天顾客支付行为次数'\\n,pay_amount_3d      DOUBLE      COMMENT '最近3天顾客支付总金额'\\n,tot_days_3d        BIGINT      COMMENT '最近3天顾客出现支付行为的天数之和'\\n) COMMENT '最近3天算法信息指标'\\nPARTITIONED BY (ds STRING ) STORED AS orc LIFECYCLE 365;\\n\\nset spark.sql.shuffle.partitions=1;\\n\\nINSERT OVERWRITE TABLE dws_alg_trd_pay_cust_3d PARTITION(ds = '${bdp.system.bizdate}')\\nSELECT\\n  date_add(concat(\\n               substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\\n               substr('${bdp.system.bizdate}', 7, 2)), 1) AS statDate,\\n  tenantId,\\n  min(shopId)                                             AS shopId,\\n  customerId,\\n  datediff(date_add(concat(substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\\n                           substr('${bdp.system.bizdate}', 7, 2)), 1),\\n           max(max_payment_time))                         AS inter_days_3d,\\n  sum(pay_quantity_1d)                                    AS pay_quantity_3d,\\n  sum(pay_amount_1d)                                      AS pay_amount_3d,\\n  sum(tot_days_1d)                                        AS tot_days_3d\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE ds BETWEEN '${key_3d}' AND '${bdp.system.bizdate}'\\nGROUP BY tenantId, customerId;\\n\",\"taskParams\":\"##Number of CPU cores need driver program is running\\n##driver.cores=1\\n\\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\\n##driver.maxResultSize=1g\\n\\n##Driver number memory used by a process\\n##driver.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##executor.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##logConf=spark.logConf\",\"scheduleConf\":\"{\\\"selfReliance\\\":0,\\\"min\\\":\\\"0\\\",\\\"hour\\\":\\\"4\\\",\\\"periodType\\\":\\\"2\\\",\\\"beginDate\\\":\\\"2001-01-01\\\",\\\"endDate\\\":\\\"2121-01-01\\\"}\",\"periodType\":null,\"scheduleStatus\":1,\"submitStatus\":0,\"modifyUserId\":90,\"createUserId\":90,\"ownerUserId\":90,\"version\":2,\"nodePid\":1193,\"taskDesc\":\"\",\"mainClass\":\"\",\"exeArgs\":\"\",\"flowId\":1050,\"createUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"modifyUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"ownerUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"taskPeriodId\":2,\"taskPeriodType\":\"天任务\",\"nodePName\":\"工作流\",\"readWriteLockVO\":{\"id\":0,\"gmtCreate\":null,\"gmtModified\":1537327159000,\"isDeleted\":0,\"lockName\":null,\"modifyUserId\":null,\"version\":1,\"projectId\":null,\"relationId\":null,\"type\":null,\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"userId\":90,\"lockVersion\":null,\"taskVariables\":[{\"paramName\":\"bdp.system.bizdate\",\"paramCommand\":\"yyyyMMdd-1\",\"type\":0},{\"paramName\":\"key_3d\",\"paramCommand\":\"$[yyyyMMdd-3]\",\"type\":1}],\"forceUpdate\":false,\"subNodes\":null,\"createModel\":0,\"operateModel\":0,\"pythonVersion\":0,\"learningType\":0,\"input\":null,\"output\":null,\"options\":null,\"flowName\":\"flow_job\",\"taskVOS\":null,\"subTaskVOS\":null,\"resourceList\":[],\"taskVersions\":[],\"cron\":\"0 0 0 * * ?\",\"merged\":false,\"cursorPosition\":{\"lineNumber\":32,\"column\":1}},\"x\":0,\"y\":80,\"value\":null,\"id\":1052}},{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1053,\"gmtCreate\":1537327261000,\"gmtModified\":1537327261000,\"isDeleted\":0,\"tenantId\":129,\"projectId\":77,\"name\":\"job_dws_alg_7d\",\"taskType\":0,\"computeType\":1,\"engineType\":1,\"sqlText\":\"CREATE TABLE if NOT EXISTS dws_alg_trd_pay_cust_7d(\\nstatDate            STRING      COMMENT '统计日期'\\n,tenantId           STRING      COMMENT '租户ID'\\n,shopId             STRING      COMMENT '门店ID'\\n,customerId         STRING      COMMENT '顾客ID'\\n,inter_days_7d      BIGINT      COMMENT '最近7天顾客最后一次发生支付行为距离当前日期多少天'\\n,pay_quantity_7d    BIGINT      COMMENT '最近7天顾客支付行为次数'\\n,pay_amount_7d      DOUBLE      COMMENT '最近7天顾客支付总金额'\\n,tot_days_7d        BIGINT      COMMENT '最近7天顾客出现支付行为的天数之和'\\n) COMMENT '最近7天算法信息指标'\\nPARTITIONED BY (ds STRING ) STORED AS orc LIFECYCLE 365;\\n\\nset spark.sql.shuffle.partitions=1;\\n\\nINSERT OVERWRITE TABLE dws_alg_trd_pay_cust_7d PARTITION(ds = '${bdp.system.bizdate}')\\nSELECT\\n  date_add(concat(\\n               substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\\n               substr('${bdp.system.bizdate}', 7, 2)), 1) AS statDate,\\n  tenantId,\\n  min(shopId)                                             AS shopId,\\n  customerId,\\n  datediff(date_add(concat(substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\\n                           substr('${bdp.system.bizdate}', 7, 2)), 1),\\n           max(max_payment_time))                         AS inter_days_7d,\\n  sum(pay_quantity_1d)                                    AS pay_quantity_7d,\\n  sum(pay_amount_1d)                                      AS pay_amount_7d,\\n  sum(tot_days_1d)                                        AS tot_days_7d\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE ds BETWEEN '${key_7d}' AND '${bdp.system.bizdate}'\\nGROUP BY tenantId, customerId;\\n\",\"taskParams\":\"##Number of CPU cores need driver program is running\\n##driver.cores=1\\n\\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\\n##driver.maxResultSize=1g\\n\\n##Driver number memory used by a process\\n##driver.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##executor.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##logConf=spark.logConf\",\"scheduleConf\":\"{\\\"selfReliance\\\":0,\\\"min\\\":\\\"0\\\",\\\"hour\\\":\\\"4\\\",\\\"periodType\\\":\\\"2\\\",\\\"beginDate\\\":\\\"2001-01-01\\\",\\\"endDate\\\":\\\"2121-01-01\\\"}\",\"periodType\":null,\"scheduleStatus\":1,\"submitStatus\":0,\"modifyUserId\":90,\"createUserId\":90,\"ownerUserId\":90,\"version\":3,\"nodePid\":1193,\"taskDesc\":\"\",\"mainClass\":\"\",\"exeArgs\":\"\",\"flowId\":1050,\"createUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"modifyUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"ownerUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"taskPeriodId\":2,\"taskPeriodType\":\"天任务\",\"nodePName\":\"工作流\",\"readWriteLockVO\":{\"id\":0,\"gmtCreate\":null,\"gmtModified\":1537327261000,\"isDeleted\":0,\"lockName\":null,\"modifyUserId\":null,\"version\":1,\"projectId\":null,\"relationId\":null,\"type\":null,\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"userId\":90,\"lockVersion\":null,\"taskVariables\":[{\"paramName\":\"bdp.system.bizdate\",\"paramCommand\":\"yyyyMMdd-1\",\"type\":0},{\"paramName\":\"key_7d\",\"paramCommand\":\"$[yyyyMMdd-7]\",\"type\":1}],\"forceUpdate\":false,\"subNodes\":null,\"createModel\":0,\"operateModel\":0,\"pythonVersion\":0,\"learningType\":0,\"input\":null,\"output\":null,\"options\":null,\"flowName\":\"flow_job\",\"taskVOS\":null,\"subTaskVOS\":null,\"resourceList\":[],\"taskVersions\":[],\"cron\":\"0 0 0 * * ?\",\"merged\":false,\"cursorPosition\":{\"lineNumber\":32,\"column\":1}},\"x\":170,\"y\":80,\"value\":null,\"id\":1053},{\"vertex\":false,\"edge\":true,\"x\":0,\"y\":0,\"value\":\"\",\"id\":\"1055\",\"source\":{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1051,\"parentId\":1193,\"name\":\"job_dws_alg_1d\",\"level\":null,\"type\":\"file\",\"taskType\":0,\"resourceType\":null,\"catalogueType\":\"TaskDevelop\",\"createUser\":\"yizhou@dtstack.com\",\"orderVal\":null,\"children\":null,\"readWriteLockVO\":{\"id\":1166,\"gmtCreate\":null,\"gmtModified\":1537327018992,\"isDeleted\":0,\"lockName\":\"1051_77_BATCH_TASK\",\"modifyUserId\":90,\"version\":1,\"projectId\":77,\"relationId\":1051,\"type\":\"BATCH_TASK\",\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"version\":0,\"operateModel\":1,\"pythonVersion\":0,\"learningType\":0,\"scriptType\":null,\"isSubTask\":0,\"scheduleStatus\":1,\"submitStatus\":0,\"catalogues\":null,\"tasks\":null,\"dependencyTasks\":null},\"x\":425,\"y\":0,\"value\":null,\"id\":1051},\"target\":{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1053,\"gmtCreate\":1537327261000,\"gmtModified\":1537327261000,\"isDeleted\":0,\"tenantId\":129,\"projectId\":77,\"name\":\"job_dws_alg_7d\",\"taskType\":0,\"computeType\":1,\"engineType\":1,\"sqlText\":\"CREATE TABLE if NOT EXISTS dws_alg_trd_pay_cust_7d(\\nstatDate            STRING      COMMENT '统计日期'\\n,tenantId           STRING      COMMENT '租户ID'\\n,shopId             STRING      COMMENT '门店ID'\\n,customerId         STRING      COMMENT '顾客ID'\\n,inter_days_7d      BIGINT      COMMENT '最近7天顾客最后一次发生支付行为距离当前日期多少天'\\n,pay_quantity_7d    BIGINT      COMMENT '最近7天顾客支付行为次数'\\n,pay_amount_7d      DOUBLE      COMMENT '最近7天顾客支付总金额'\\n,tot_days_7d        BIGINT      COMMENT '最近7天顾客出现支付行为的天数之和'\\n) COMMENT '最近7天算法信息指标'\\nPARTITIONED BY (ds STRING ) STORED AS orc LIFECYCLE 365;\\n\\nset spark.sql.shuffle.partitions=1;\\n\\nINSERT OVERWRITE TABLE dws_alg_trd_pay_cust_7d PARTITION(ds = '${bdp.system.bizdate}')\\nSELECT\\n  date_add(concat(\\n               substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\\n               substr('${bdp.system.bizdate}', 7, 2)), 1) AS statDate,\\n  tenantId,\\n  min(shopId)                                             AS shopId,\\n  customerId,\\n  datediff(date_add(concat(substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\\n                           substr('${bdp.system.bizdate}', 7, 2)), 1),\\n           max(max_payment_time))                         AS inter_days_7d,\\n  sum(pay_quantity_1d)                                    AS pay_quantity_7d,\\n  sum(pay_amount_1d)                                      AS pay_amount_7d,\\n  sum(tot_days_1d)                                        AS tot_days_7d\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE ds BETWEEN '${key_7d}' AND '${bdp.system.bizdate}'\\nGROUP BY tenantId, customerId;\\n\",\"taskParams\":\"##Number of CPU cores need driver program is running\\n##driver.cores=1\\n\\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\\n##driver.maxResultSize=1g\\n\\n##Driver number memory used by a process\\n##driver.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##executor.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##logConf=spark.logConf\",\"scheduleConf\":\"{\\\"selfReliance\\\":0,\\\"min\\\":\\\"0\\\",\\\"hour\\\":\\\"4\\\",\\\"periodType\\\":\\\"2\\\",\\\"beginDate\\\":\\\"2001-01-01\\\",\\\"endDate\\\":\\\"2121-01-01\\\"}\",\"periodType\":null,\"scheduleStatus\":1,\"submitStatus\":0,\"modifyUserId\":90,\"createUserId\":90,\"ownerUserId\":90,\"version\":3,\"nodePid\":1193,\"taskDesc\":\"\",\"mainClass\":\"\",\"exeArgs\":\"\",\"flowId\":1050,\"createUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"modifyUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"ownerUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"taskPeriodId\":2,\"taskPeriodType\":\"天任务\",\"nodePName\":\"工作流\",\"readWriteLockVO\":{\"id\":0,\"gmtCreate\":null,\"gmtModified\":1537327261000,\"isDeleted\":0,\"lockName\":null,\"modifyUserId\":null,\"version\":1,\"projectId\":null,\"relationId\":null,\"type\":null,\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"userId\":90,\"lockVersion\":null,\"taskVariables\":[{\"paramName\":\"bdp.system.bizdate\",\"paramCommand\":\"yyyyMMdd-1\",\"type\":0},{\"paramName\":\"key_7d\",\"paramCommand\":\"$[yyyyMMdd-7]\",\"type\":1}],\"forceUpdate\":false,\"subNodes\":null,\"createModel\":0,\"operateModel\":0,\"pythonVersion\":0,\"learningType\":0,\"input\":null,\"output\":null,\"options\":null,\"flowName\":\"flow_job\",\"taskVOS\":null,\"subTaskVOS\":null,\"resourceList\":[],\"taskVersions\":[],\"cron\":\"0 0 0 * * ?\",\"merged\":false,\"cursorPosition\":{\"lineNumber\":32,\"column\":1}},\"x\":170,\"y\":80,\"value\":null,\"id\":1053}},{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1054,\"parentId\":1193,\"name\":\"job_dws_alg_15d\",\"level\":null,\"type\":\"file\",\"taskType\":0,\"resourceType\":null,\"catalogueType\":\"TaskDevelop\",\"createUser\":\"yizhou@dtstack.com\",\"orderVal\":null,\"children\":null,\"readWriteLockVO\":{\"id\":1169,\"gmtCreate\":null,\"gmtModified\":1537327326523,\"isDeleted\":0,\"lockName\":\"1054_77_BATCH_TASK\",\"modifyUserId\":90,\"version\":1,\"projectId\":77,\"relationId\":1054,\"type\":\"BATCH_TASK\",\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"version\":0,\"operateModel\":1,\"pythonVersion\":0,\"learningType\":0,\"scriptType\":null,\"isSubTask\":0,\"scheduleStatus\":1,\"submitStatus\":0,\"catalogues\":null,\"tasks\":null,\"dependencyTasks\":null},\"x\":340,\"y\":80,\"value\":null,\"id\":\"1056\"},{\"vertex\":false,\"edge\":true,\"x\":0,\"y\":0,\"value\":\"\",\"id\":\"1057\",\"source\":{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1051,\"parentId\":1193,\"name\":\"job_dws_alg_1d\",\"level\":null,\"type\":\"file\",\"taskType\":0,\"resourceType\":null,\"catalogueType\":\"TaskDevelop\",\"createUser\":\"yizhou@dtstack.com\",\"orderVal\":null,\"children\":null,\"readWriteLockVO\":{\"id\":1166,\"gmtCreate\":null,\"gmtModified\":1537327018992,\"isDeleted\":0,\"lockName\":\"1051_77_BATCH_TASK\",\"modifyUserId\":90,\"version\":1,\"projectId\":77,\"relationId\":1051,\"type\":\"BATCH_TASK\",\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"version\":0,\"operateModel\":1,\"pythonVersion\":0,\"learningType\":0,\"scriptType\":null,\"isSubTask\":0,\"scheduleStatus\":1,\"submitStatus\":0,\"catalogues\":null,\"tasks\":null,\"dependencyTasks\":null},\"x\":425,\"y\":0,\"value\":null,\"id\":1051},\"target\":{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1054,\"parentId\":1193,\"name\":\"job_dws_alg_15d\",\"level\":null,\"type\":\"file\",\"taskType\":0,\"resourceType\":null,\"catalogueType\":\"TaskDevelop\",\"createUser\":\"yizhou@dtstack.com\",\"orderVal\":null,\"children\":null,\"readWriteLockVO\":{\"id\":1169,\"gmtCreate\":null,\"gmtModified\":1537327326523,\"isDeleted\":0,\"lockName\":\"1054_77_BATCH_TASK\",\"modifyUserId\":90,\"version\":1,\"projectId\":77,\"relationId\":1054,\"type\":\"BATCH_TASK\",\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"version\":0,\"operateModel\":1,\"pythonVersion\":0,\"learningType\":0,\"scriptType\":null,\"isSubTask\":0,\"scheduleStatus\":1,\"submitStatus\":0,\"catalogues\":null,\"tasks\":null,\"dependencyTasks\":null},\"x\":340,\"y\":80,\"value\":null,\"id\":\"1056\"}},{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1055,\"gmtCreate\":1537327392000,\"gmtModified\":1537327392000,\"isDeleted\":0,\"tenantId\":129,\"projectId\":77,\"name\":\"job_dws_alg_30d\",\"taskType\":0,\"computeType\":1,\"engineType\":1,\"sqlText\":\"\",\"taskParams\":\"##Number of CPU cores need driver program is running\\n##driver.cores=1\\n\\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\\n##driver.maxResultSize=1g\\n\\n##Driver number memory used by a process\\n##driver.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##executor.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##logConf=spark.logConf\",\"scheduleConf\":\"{\\\"selfReliance\\\":false, \\\"min\\\":0,\\\"hour\\\":0,\\\"periodType\\\":\\\"2\\\",\\\"beginDate\\\":\\\"2001-01-01\\\",\\\"endDate\\\":\\\"2121-01-01\\\"}\",\"periodType\":null,\"scheduleStatus\":1,\"submitStatus\":0,\"modifyUserId\":90,\"createUserId\":90,\"ownerUserId\":90,\"version\":0,\"nodePid\":1193,\"taskDesc\":\"\",\"mainClass\":\"\",\"exeArgs\":\"\",\"flowId\":1050,\"createUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"modifyUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"ownerUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"taskPeriodId\":2,\"taskPeriodType\":\"天任务\",\"nodePName\":\"工作流\",\"readWriteLockVO\":{\"id\":1170,\"gmtCreate\":1537327392000,\"gmtModified\":1537327392000,\"isDeleted\":0,\"lockName\":\"1055_77_BATCH_TASK\",\"modifyUserId\":90,\"version\":1,\"projectId\":77,\"relationId\":1055,\"type\":\"BATCH_TASK\",\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"userId\":90,\"lockVersion\":null,\"taskVariables\":[],\"forceUpdate\":false,\"subNodes\":null,\"createModel\":0,\"operateModel\":0,\"pythonVersion\":0,\"learningType\":0,\"input\":null,\"output\":null,\"options\":null,\"flowName\":\"flow_job\",\"taskVOS\":null,\"subTaskVOS\":null,\"resourceList\":[],\"taskVersions\":[],\"cron\":\"0 0 0 * * ?\"},\"x\":510,\"y\":80,\"value\":null,\"id\":\"1058\"},{\"vertex\":false,\"edge\":true,\"x\":0,\"y\":0,\"value\":\"\",\"id\":\"1059\",\"source\":{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1051,\"parentId\":1193,\"name\":\"job_dws_alg_1d\",\"level\":null,\"type\":\"file\",\"taskType\":0,\"resourceType\":null,\"catalogueType\":\"TaskDevelop\",\"createUser\":\"yizhou@dtstack.com\",\"orderVal\":null,\"children\":null,\"readWriteLockVO\":{\"id\":1166,\"gmtCreate\":null,\"gmtModified\":1537327018992,\"isDeleted\":0,\"lockName\":\"1051_77_BATCH_TASK\",\"modifyUserId\":90,\"version\":1,\"projectId\":77,\"relationId\":1051,\"type\":\"BATCH_TASK\",\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"version\":0,\"operateModel\":1,\"pythonVersion\":0,\"learningType\":0,\"scriptType\":null,\"isSubTask\":0,\"scheduleStatus\":1,\"submitStatus\":0,\"catalogues\":null,\"tasks\":null,\"dependencyTasks\":null},\"x\":425,\"y\":0,\"value\":null,\"id\":1051},\"target\":{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1055,\"gmtCreate\":1537327392000,\"gmtModified\":1537327392000,\"isDeleted\":0,\"tenantId\":129,\"projectId\":77,\"name\":\"job_dws_alg_30d\",\"taskType\":0,\"computeType\":1,\"engineType\":1,\"sqlText\":\"\",\"taskParams\":\"##Number of CPU cores need driver program is running\\n##driver.cores=1\\n\\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\\n##driver.maxResultSize=1g\\n\\n##Driver number memory used by a process\\n##driver.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##executor.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##logConf=spark.logConf\",\"scheduleConf\":\"{\\\"selfReliance\\\":false, \\\"min\\\":0,\\\"hour\\\":0,\\\"periodType\\\":\\\"2\\\",\\\"beginDate\\\":\\\"2001-01-01\\\",\\\"endDate\\\":\\\"2121-01-01\\\"}\",\"periodType\":null,\"scheduleStatus\":1,\"submitStatus\":0,\"modifyUserId\":90,\"createUserId\":90,\"ownerUserId\":90,\"version\":0,\"nodePid\":1193,\"taskDesc\":\"\",\"mainClass\":\"\",\"exeArgs\":\"\",\"flowId\":1050,\"createUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"modifyUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"ownerUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"taskPeriodId\":2,\"taskPeriodType\":\"天任务\",\"nodePName\":\"工作流\",\"readWriteLockVO\":{\"id\":1170,\"gmtCreate\":1537327392000,\"gmtModified\":1537327392000,\"isDeleted\":0,\"lockName\":\"1055_77_BATCH_TASK\",\"modifyUserId\":90,\"version\":1,\"projectId\":77,\"relationId\":1055,\"type\":\"BATCH_TASK\",\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"userId\":90,\"lockVersion\":null,\"taskVariables\":[],\"forceUpdate\":false,\"subNodes\":null,\"createModel\":0,\"operateModel\":0,\"pythonVersion\":0,\"learningType\":0,\"input\":null,\"output\":null,\"options\":null,\"flowName\":\"flow_job\",\"taskVOS\":null,\"subTaskVOS\":null,\"resourceList\":[],\"taskVersions\":[],\"cron\":\"0 0 0 * * ?\"},\"x\":510,\"y\":80,\"value\":null,\"id\":\"1058\"}},{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1056,\"gmtCreate\":1537327469000,\"gmtModified\":1537327469000,\"isDeleted\":0,\"tenantId\":129,\"projectId\":77,\"name\":\"job_dws_alg_std\",\"taskType\":0,\"computeType\":1,\"engineType\":1,\"sqlText\":\"CREATE TABLE if NOT EXISTS dws_alg_trd_pay_cust_std(\\nstatDate                STRING      COMMENT '统计日期'\\n,tenantId               STRING      COMMENT '租户ID'\\n,shopId                 STRING      COMMENT '门店ID'\\n,customerId             STRING      COMMENT '顾客ID'\\n,inter_days_std         BIGINT      COMMENT '历史至今顾客最后一次发生支付行为距离当前日期多少天'\\n,pay_quantity_std       BIGINT      COMMENT '历史至今顾客支付行为次数'\\n,pay_amount_std         DOUBLE      COMMENT '历史至今顾客支付总金额'\\n,tot_days_std           BIGINT      COMMENT '历史至今顾客出现支付行为的天数之和'\\n) COMMENT '历史至今算法信息指标'\\nPARTITIONED BY (ds STRING ) STORED AS orc LIFECYCLE 365;\\n\\nset spark.sql.shuffle.partitions=1;\\n\\n-- INSERT OVERWRITE TABLE dws_alg_trd_pay_cust_std PARTITION(ds = '${bdp.system.bizdate}')\\n-- SELECT\\n--   date_add(concat(substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-', substr('${bdp.system.bizdate}', 7, 2)),1)                                AS statDate,\\n--   coalesce(d.tenantId, std.tenantId)                                 AS tenantId,\\n--   coalesce(d.shopId, std.shopId)                                     AS shopId,\\n--   coalesce(d.customerId, std.customerId)                             AS customerId,\\n--   if(coalesce(d.inter_days_1d,0) = 1, 1, std.inter_days_std + 1)     AS inter_days_std,\\n--   coalesce(d.pay_quantity_1d, 0) + coalesce(std.pay_quantity_std, 0) AS pay_quantity_std,\\n--   coalesce(d.pay_amount_1d, 0) + coalesce(std.pay_amount_std, 0)     AS pay_amount_std,\\n--   coalesce(d.tot_days_1d, 0) + coalesce(std.tot_days_std, 0)         AS tot_days_std\\n-- FROM (\\n--   SELECT \\n--   tenantId,\\n--   shopId,\\n--   customerId,\\n--   inter_days_1d,\\n--   pay_quantity_1d,\\n--   pay_amount_1d,\\n--   tot_days_1d\\n--   FROM \\n--   dws_alg_trd_pay_cust_1d WHERE ds = '${bdp.system.bizdate}') d\\n-- FULL OUTER JOIN (\\n--   SELECT \\n--   tenantId,\\n--   shopId,\\n--   customerId,\\n--   inter_days_std,\\n--   pay_quantity_std,\\n--   pay_amount_std,\\n--   tot_days_std \\n--   FROM dws_alg_trd_pay_cust_std WHERE ds = '${key_2d}') std\\n-- ON d.tenantId = std.tenantId AND d.customerId = std.customerId;\\n\\n\\nINSERT OVERWRITE TABLE dws_alg_trd_pay_cust_std PARTITION(ds = '${bdp.system.bizdate}')\\nSELECT\\n  date_add(concat(\\n               substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\\n               substr('${bdp.system.bizdate}', 7, 2)), 1) AS statDate,\\n  tenantId,\\n  min(shopId)                                             AS shopId,\\n  customerId,\\n  datediff(date_add(concat(substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\\n                           substr('${bdp.system.bizdate}', 7, 2)), 1),\\n           max(max_payment_time))                         AS inter_days_std,\\n  sum(pay_quantity_1d)                                    AS pay_quantity_std,\\n  sum(pay_amount_1d)                                      AS pay_amount_std,\\n  sum(tot_days_1d)                                        AS tot_days_std\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE ds <= '${bdp.system.bizdate}'\\nGROUP BY tenantId, customerId;\\n\",\"taskParams\":\"##Number of CPU cores need driver program is running\\n##driver.cores=1\\n\\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\\n##driver.maxResultSize=1g\\n\\n##Driver number memory used by a process\\n##driver.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##executor.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##logConf=spark.logConf\",\"scheduleConf\":\"{\\\"selfReliance\\\":0,\\\"min\\\":\\\"0\\\",\\\"hour\\\":\\\"4\\\",\\\"periodType\\\":\\\"2\\\",\\\"beginDate\\\":\\\"2001-01-01\\\",\\\"endDate\\\":\\\"2121-01-01\\\"}\",\"periodType\":null,\"scheduleStatus\":1,\"submitStatus\":0,\"modifyUserId\":90,\"createUserId\":90,\"ownerUserId\":90,\"version\":1,\"nodePid\":1193,\"taskDesc\":\"\",\"mainClass\":\"\",\"exeArgs\":\"\",\"flowId\":1050,\"createUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"modifyUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"ownerUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"taskPeriodId\":2,\"taskPeriodType\":\"天任务\",\"nodePName\":\"工作流\",\"readWriteLockVO\":{\"id\":0,\"gmtCreate\":null,\"gmtModified\":1537327468000,\"isDeleted\":0,\"lockName\":null,\"modifyUserId\":null,\"version\":1,\"projectId\":null,\"relationId\":null,\"type\":null,\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"userId\":90,\"lockVersion\":null,\"taskVariables\":[{\"paramName\":\"bdp.system.bizdate\",\"paramCommand\":\"yyyyMMdd-1\",\"type\":0}],\"forceUpdate\":false,\"subNodes\":null,\"createModel\":0,\"operateModel\":0,\"pythonVersion\":0,\"learningType\":0,\"input\":null,\"output\":null,\"options\":null,\"flowName\":\"flow_job\",\"taskVOS\":null,\"subTaskVOS\":null,\"resourceList\":[],\"taskVersions\":[],\"cron\":\"0 0 0 * * ?\",\"merged\":false,\"cursorPosition\":{\"lineNumber\":66,\"column\":1}},\"x\":680,\"y\":80,\"value\":null,\"id\":\"1060\"},{\"vertex\":false,\"edge\":true,\"x\":0,\"y\":0,\"value\":\"\",\"id\":\"1061\",\"source\":{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1051,\"parentId\":1193,\"name\":\"job_dws_alg_1d\",\"level\":null,\"type\":\"file\",\"taskType\":0,\"resourceType\":null,\"catalogueType\":\"TaskDevelop\",\"createUser\":\"yizhou@dtstack.com\",\"orderVal\":null,\"children\":null,\"readWriteLockVO\":{\"id\":1166,\"gmtCreate\":null,\"gmtModified\":1537327018992,\"isDeleted\":0,\"lockName\":\"1051_77_BATCH_TASK\",\"modifyUserId\":90,\"version\":1,\"projectId\":77,\"relationId\":1051,\"type\":\"BATCH_TASK\",\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"version\":0,\"operateModel\":1,\"pythonVersion\":0,\"learningType\":0,\"scriptType\":null,\"isSubTask\":0,\"scheduleStatus\":1,\"submitStatus\":0,\"catalogues\":null,\"tasks\":null,\"dependencyTasks\":null},\"x\":425,\"y\":0,\"value\":null,\"id\":1051},\"target\":{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1056,\"gmtCreate\":1537327469000,\"gmtModified\":1537327469000,\"isDeleted\":0,\"tenantId\":129,\"projectId\":77,\"name\":\"job_dws_alg_std\",\"taskType\":0,\"computeType\":1,\"engineType\":1,\"sqlText\":\"CREATE TABLE if NOT EXISTS dws_alg_trd_pay_cust_std(\\nstatDate                STRING      COMMENT '统计日期'\\n,tenantId               STRING      COMMENT '租户ID'\\n,shopId                 STRING      COMMENT '门店ID'\\n,customerId             STRING      COMMENT '顾客ID'\\n,inter_days_std         BIGINT      COMMENT '历史至今顾客最后一次发生支付行为距离当前日期多少天'\\n,pay_quantity_std       BIGINT      COMMENT '历史至今顾客支付行为次数'\\n,pay_amount_std         DOUBLE      COMMENT '历史至今顾客支付总金额'\\n,tot_days_std           BIGINT      COMMENT '历史至今顾客出现支付行为的天数之和'\\n) COMMENT '历史至今算法信息指标'\\nPARTITIONED BY (ds STRING ) STORED AS orc LIFECYCLE 365;\\n\\nset spark.sql.shuffle.partitions=1;\\n\\n-- INSERT OVERWRITE TABLE dws_alg_trd_pay_cust_std PARTITION(ds = '${bdp.system.bizdate}')\\n-- SELECT\\n--   date_add(concat(substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-', substr('${bdp.system.bizdate}', 7, 2)),1)                                AS statDate,\\n--   coalesce(d.tenantId, std.tenantId)                                 AS tenantId,\\n--   coalesce(d.shopId, std.shopId)                                     AS shopId,\\n--   coalesce(d.customerId, std.customerId)                             AS customerId,\\n--   if(coalesce(d.inter_days_1d,0) = 1, 1, std.inter_days_std + 1)     AS inter_days_std,\\n--   coalesce(d.pay_quantity_1d, 0) + coalesce(std.pay_quantity_std, 0) AS pay_quantity_std,\\n--   coalesce(d.pay_amount_1d, 0) + coalesce(std.pay_amount_std, 0)     AS pay_amount_std,\\n--   coalesce(d.tot_days_1d, 0) + coalesce(std.tot_days_std, 0)         AS tot_days_std\\n-- FROM (\\n--   SELECT \\n--   tenantId,\\n--   shopId,\\n--   customerId,\\n--   inter_days_1d,\\n--   pay_quantity_1d,\\n--   pay_amount_1d,\\n--   tot_days_1d\\n--   FROM \\n--   dws_alg_trd_pay_cust_1d WHERE ds = '${bdp.system.bizdate}') d\\n-- FULL OUTER JOIN (\\n--   SELECT \\n--   tenantId,\\n--   shopId,\\n--   customerId,\\n--   inter_days_std,\\n--   pay_quantity_std,\\n--   pay_amount_std,\\n--   tot_days_std \\n--   FROM dws_alg_trd_pay_cust_std WHERE ds = '${key_2d}') std\\n-- ON d.tenantId = std.tenantId AND d.customerId = std.customerId;\\n\\n\\nINSERT OVERWRITE TABLE dws_alg_trd_pay_cust_std PARTITION(ds = '${bdp.system.bizdate}')\\nSELECT\\n  date_add(concat(\\n               substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\\n               substr('${bdp.system.bizdate}', 7, 2)), 1) AS statDate,\\n  tenantId,\\n  min(shopId)                                             AS shopId,\\n  customerId,\\n  datediff(date_add(concat(substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\\n                           substr('${bdp.system.bizdate}', 7, 2)), 1),\\n           max(max_payment_time))                         AS inter_days_std,\\n  sum(pay_quantity_1d)                                    AS pay_quantity_std,\\n  sum(pay_amount_1d)                                      AS pay_amount_std,\\n  sum(tot_days_1d)                                        AS tot_days_std\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE ds <= '${bdp.system.bizdate}'\\nGROUP BY tenantId, customerId;\\n\",\"taskParams\":\"##Number of CPU cores need driver program is running\\n##driver.cores=1\\n\\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\\n##driver.maxResultSize=1g\\n\\n##Driver number memory used by a process\\n##driver.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##executor.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##logConf=spark.logConf\",\"scheduleConf\":\"{\\\"selfReliance\\\":0,\\\"min\\\":\\\"0\\\",\\\"hour\\\":\\\"4\\\",\\\"periodType\\\":\\\"2\\\",\\\"beginDate\\\":\\\"2001-01-01\\\",\\\"endDate\\\":\\\"2121-01-01\\\"}\",\"periodType\":null,\"scheduleStatus\":1,\"submitStatus\":0,\"modifyUserId\":90,\"createUserId\":90,\"ownerUserId\":90,\"version\":1,\"nodePid\":1193,\"taskDesc\":\"\",\"mainClass\":\"\",\"exeArgs\":\"\",\"flowId\":1050,\"createUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"modifyUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"ownerUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"taskPeriodId\":2,\"taskPeriodType\":\"天任务\",\"nodePName\":\"工作流\",\"readWriteLockVO\":{\"id\":0,\"gmtCreate\":null,\"gmtModified\":1537327468000,\"isDeleted\":0,\"lockName\":null,\"modifyUserId\":null,\"version\":1,\"projectId\":null,\"relationId\":null,\"type\":null,\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"userId\":90,\"lockVersion\":null,\"taskVariables\":[{\"paramName\":\"bdp.system.bizdate\",\"paramCommand\":\"yyyyMMdd-1\",\"type\":0}],\"forceUpdate\":false,\"subNodes\":null,\"createModel\":0,\"operateModel\":0,\"pythonVersion\":0,\"learningType\":0,\"input\":null,\"output\":null,\"options\":null,\"flowName\":\"flow_job\",\"taskVOS\":null,\"subTaskVOS\":null,\"resourceList\":[],\"taskVersions\":[],\"cron\":\"0 0 0 * * ?\",\"merged\":false,\"cursorPosition\":{\"lineNumber\":66,\"column\":1}},\"x\":680,\"y\":80,\"value\":null,\"id\":\"1060\"}},{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1057,\"gmtCreate\":1537327573000,\"gmtModified\":1537327573000,\"isDeleted\":0,\"tenantId\":129,\"projectId\":77,\"name\":\"job_dws_alg_label\",\"taskType\":0,\"computeType\":1,\"engineType\":1,\"sqlText\":\"CREATE TABLE if NOT EXISTS dws_alg_cust_label(\\nstatDate                STRING      COMMENT '统计日期'\\n,tenantId               STRING      COMMENT '租户ID'\\n,shopId                 STRING      COMMENT '门店ID'\\n,customerId             STRING      COMMENT '顾客ID'\\n,label                  BIGINT      COMMENT '顾客未来七天内是否消费：1-是，0-否 (距离今日不足七天算最近区间)'\\n) COMMENT '当日顾客未来七天内是否消费标记'\\nPARTITIONED BY (ds STRING ) STORED AS orc LIFECYCLE 365;\\n\\n\\nset spark.sql.shuffle.partitions=1;\\n\\n-------- 前一天 ----------\\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${bdp.system.bizdate}')\\nSELECT\\n  date_add(concat(substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-', substr('${bdp.system.bizdate}', 7, 2)),1)        AS statDate,\\n  d.tenantId,\\n  d.shopId,\\n  d.customerId,\\n  if(d.cnt > 0, 1, 0)            AS lable\\nFROM(\\nSELECT\\n  tenantId,\\n  min(shopId)                    AS shopId,\\n  customerId,\\n  count(1)                       AS cnt\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE (ds BETWEEN '${bdp.system.bizdate}' AND '${key_f5d}') AND isnotnull(customerId) \\nGROUP BY tenantId, customerId) d;\\n\\n\\n-------- 前两天 ---------\\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_2d}')\\nSELECT\\n  date_add(concat(substr('${key_2d}', 0, 4), '-', substr('${key_2d}', 5, 2), '-', substr('${key_2d}', 7, 2)),1)        AS statDate,\\n  d.tenantId,\\n  d.shopId,\\n  d.customerId,\\n  if(d.cnt > 0, 1, 0)            AS lable\\nFROM(\\nSELECT\\n  tenantId,\\n  min(shopId)                    AS shopId,\\n  customerId,\\n  count(1)                       AS cnt\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE (ds BETWEEN '${key_2d}' AND '${key_f4d}') AND isnotnull(customerId) \\nGROUP BY tenantId, customerId) d;\\n\\n\\n-------- 前三天 ---------\\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_3d}')\\nSELECT\\n  date_add(concat(substr('${key_3d}', 0, 4), '-', substr('${key_3d}', 5, 2), '-', substr('${key_3d}', 7, 2)),1)        AS statDate,\\n  d.tenantId,\\n  d.shopId,\\n  d.customerId,\\n  if(d.cnt > 0, 1, 0)            AS lable\\nFROM(\\nSELECT\\n  tenantId,\\n  min(shopId)                    AS shopId,\\n  customerId,\\n  count(1)                       AS cnt\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE (ds BETWEEN '${key_3d}' AND '${key_f3d}') AND isnotnull(customerId) \\nGROUP BY tenantId, customerId) d;\\n\\n-------- 前四天 ---------\\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_4d}')\\nSELECT\\n  date_add(concat(substr('${key_4d}', 0, 4), '-', substr('${key_4d}', 5, 2), '-', substr('${key_4d}', 7, 2)),1)        AS statDate,\\n  d.tenantId,\\n  d.shopId,\\n  d.customerId,\\n  if(d.cnt > 0, 1, 0)            AS lable\\nFROM(\\nSELECT\\n  tenantId,\\n  min(shopId)                    AS shopId,\\n  customerId,\\n  count(1)                       AS cnt\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE (ds BETWEEN '${key_4d}' AND '${key_f2d}') AND isnotnull(customerId) \\nGROUP BY tenantId, customerId) d;\\n\\n\\n-------- 前五天 ---------\\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_5d}')\\nSELECT\\n  date_add(concat(substr('${key_5d}', 0, 4), '-', substr('${key_5d}', 5, 2), '-', substr('${key_5d}', 7, 2)),1)        AS statDate,\\n  d.tenantId,\\n  d.shopId,\\n  d.customerId,\\n  if(d.cnt > 0, 1, 0)            AS lable\\nFROM(\\nSELECT\\n  tenantId,\\n  min(shopId)                    AS shopId,\\n  customerId,\\n  count(1)                       AS cnt\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE (ds BETWEEN '${key_5d}' AND '${key_f1d}') AND isnotnull(customerId) \\nGROUP BY tenantId, customerId) d;\\n\\n\\n-------- 前六天 ---------\\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_6d}')\\nSELECT\\n  date_add(concat(substr('${key_6d}', 0, 4), '-', substr('${key_6d}', 5, 2), '-', substr('${key_6d}', 7, 2)),1)        AS statDate,\\n  d.tenantId,\\n  d.shopId,\\n  d.customerId,\\n  if(d.cnt > 0, 1, 0)            AS lable\\nFROM(\\nSELECT\\n  tenantId,\\n  min(shopId)                    AS shopId,\\n  customerId,\\n  count(1)                       AS cnt\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE (ds BETWEEN '${key_6d}' AND substr('${bdp.system.cyctime}',0,8)) AND isnotnull(customerId) \\nGROUP BY tenantId, customerId) d;\\n\\n\\n-------- 前七天 ---------\\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_7d}')\\nSELECT\\n  date_add(concat(substr('${key_7d}', 0, 4), '-', substr('${key_7d}', 5, 2), '-', substr('${key_7d}', 7, 2)),1)        AS statDate,\\n  d.tenantId,\\n  d.shopId,\\n  d.customerId,\\n  if(d.cnt > 0, 1, 0)            AS lable\\nFROM(\\nSELECT\\n  tenantId,\\n  min(shopId)                    AS shopId,\\n  customerId,\\n  count(1)                       AS cnt\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE (ds BETWEEN '${key_7d}' AND '${bdp.system.bizdate}') AND isnotnull(customerId) \\nGROUP BY tenantId, customerId) d;\\n\\n\",\"taskParams\":\"##Number of CPU cores need driver program is running\\n##driver.cores=1\\n\\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\\n##driver.maxResultSize=1g\\n\\n##Driver number memory used by a process\\n##driver.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##executor.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##logConf=spark.logConf\",\"scheduleConf\":\"{\\\"selfReliance\\\":false, \\\"min\\\":0,\\\"hour\\\":0,\\\"periodType\\\":\\\"2\\\",\\\"beginDate\\\":\\\"2001-01-01\\\",\\\"endDate\\\":\\\"2121-01-01\\\"}\",\"periodType\":null,\"scheduleStatus\":1,\"submitStatus\":0,\"modifyUserId\":90,\"createUserId\":90,\"ownerUserId\":90,\"version\":1,\"nodePid\":1193,\"taskDesc\":\"\",\"mainClass\":\"\",\"exeArgs\":\"\",\"flowId\":1050,\"createUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"modifyUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"ownerUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"taskPeriodId\":2,\"taskPeriodType\":\"天任务\",\"nodePName\":\"工作流\",\"readWriteLockVO\":{\"id\":0,\"gmtCreate\":null,\"gmtModified\":1537327573000,\"isDeleted\":0,\"lockName\":null,\"modifyUserId\":null,\"version\":1,\"projectId\":null,\"relationId\":null,\"type\":null,\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"userId\":90,\"lockVersion\":null,\"taskVariables\":[{\"paramName\":\"bdp.system.bizdate\",\"paramCommand\":\"yyyyMMdd-1\",\"type\":0},{\"paramName\":\"key_f5d\",\"paramCommand\":\"$[yyyyMMdd+5]\",\"type\":1},{\"paramName\":\"key_2d\",\"paramCommand\":\"$[yyyyMMdd-2]\",\"type\":1},{\"paramName\":\"key_f4d\",\"paramCommand\":\"$[yyyyMMdd+4]\",\"type\":1},{\"paramName\":\"key_3d\",\"paramCommand\":\"$[yyyyMMdd-3]\",\"type\":1},{\"paramName\":\"key_f3d\",\"paramCommand\":\"$[yyyyMMdd+3]\",\"type\":1},{\"paramName\":\"key_4d\",\"paramCommand\":\"$[yyyyMMdd-4]\",\"type\":1},{\"paramName\":\"key_f2d\",\"paramCommand\":\"$[yyyyMMdd+2]\",\"type\":1},{\"paramName\":\"key_5d\",\"paramCommand\":\"$[yyyyMMdd-5]\",\"type\":1},{\"paramName\":\"key_f1d\",\"paramCommand\":\"$[yyyyMMdd+1]\",\"type\":1},{\"paramName\":\"key_6d\",\"paramCommand\":\"$[yyyyMMdd-6]\",\"type\":1},{\"paramName\":\"bdp.system.cyctime\",\"paramCommand\":\"yyyyMMddHHmmss\",\"type\":0},{\"paramName\":\"key_7d\",\"paramCommand\":\"$[yyyyMMdd-7]\",\"type\":1}],\"forceUpdate\":false,\"subNodes\":null,\"createModel\":0,\"operateModel\":0,\"pythonVersion\":0,\"learningType\":0,\"input\":null,\"output\":null,\"options\":null,\"flowName\":\"flow_job\",\"taskVOS\":null,\"subTaskVOS\":null,\"resourceList\":[],\"taskVersions\":[],\"cron\":\"0 0 0 * * ?\",\"merged\":false,\"cursorPosition\":{\"lineNumber\":144,\"column\":1}},\"x\":850,\"y\":80,\"value\":null,\"id\":\"1062\"},{\"vertex\":false,\"edge\":true,\"x\":0,\"y\":0,\"value\":\"\",\"id\":\"1063\",\"source\":{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1051,\"parentId\":1193,\"name\":\"job_dws_alg_1d\",\"level\":null,\"type\":\"file\",\"taskType\":0,\"resourceType\":null,\"catalogueType\":\"TaskDevelop\",\"createUser\":\"yizhou@dtstack.com\",\"orderVal\":null,\"children\":null,\"readWriteLockVO\":{\"id\":1166,\"gmtCreate\":null,\"gmtModified\":1537327018992,\"isDeleted\":0,\"lockName\":\"1051_77_BATCH_TASK\",\"modifyUserId\":90,\"version\":1,\"projectId\":77,\"relationId\":1051,\"type\":\"BATCH_TASK\",\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"version\":0,\"operateModel\":1,\"pythonVersion\":0,\"learningType\":0,\"scriptType\":null,\"isSubTask\":0,\"scheduleStatus\":1,\"submitStatus\":0,\"catalogues\":null,\"tasks\":null,\"dependencyTasks\":null},\"x\":425,\"y\":0,\"value\":null,\"id\":1051},\"target\":{\"vertex\":true,\"edge\":false,\"data\":{\"id\":1057,\"gmtCreate\":1537327573000,\"gmtModified\":1537327573000,\"isDeleted\":0,\"tenantId\":129,\"projectId\":77,\"name\":\"job_dws_alg_label\",\"taskType\":0,\"computeType\":1,\"engineType\":1,\"sqlText\":\"CREATE TABLE if NOT EXISTS dws_alg_cust_label(\\nstatDate                STRING      COMMENT '统计日期'\\n,tenantId               STRING      COMMENT '租户ID'\\n,shopId                 STRING      COMMENT '门店ID'\\n,customerId             STRING      COMMENT '顾客ID'\\n,label                  BIGINT      COMMENT '顾客未来七天内是否消费：1-是，0-否 (距离今日不足七天算最近区间)'\\n) COMMENT '当日顾客未来七天内是否消费标记'\\nPARTITIONED BY (ds STRING ) STORED AS orc LIFECYCLE 365;\\n\\n\\nset spark.sql.shuffle.partitions=1;\\n\\n-------- 前一天 ----------\\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${bdp.system.bizdate}')\\nSELECT\\n  date_add(concat(substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-', substr('${bdp.system.bizdate}', 7, 2)),1)        AS statDate,\\n  d.tenantId,\\n  d.shopId,\\n  d.customerId,\\n  if(d.cnt > 0, 1, 0)            AS lable\\nFROM(\\nSELECT\\n  tenantId,\\n  min(shopId)                    AS shopId,\\n  customerId,\\n  count(1)                       AS cnt\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE (ds BETWEEN '${bdp.system.bizdate}' AND '${key_f5d}') AND isnotnull(customerId) \\nGROUP BY tenantId, customerId) d;\\n\\n\\n-------- 前两天 ---------\\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_2d}')\\nSELECT\\n  date_add(concat(substr('${key_2d}', 0, 4), '-', substr('${key_2d}', 5, 2), '-', substr('${key_2d}', 7, 2)),1)        AS statDate,\\n  d.tenantId,\\n  d.shopId,\\n  d.customerId,\\n  if(d.cnt > 0, 1, 0)            AS lable\\nFROM(\\nSELECT\\n  tenantId,\\n  min(shopId)                    AS shopId,\\n  customerId,\\n  count(1)                       AS cnt\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE (ds BETWEEN '${key_2d}' AND '${key_f4d}') AND isnotnull(customerId) \\nGROUP BY tenantId, customerId) d;\\n\\n\\n-------- 前三天 ---------\\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_3d}')\\nSELECT\\n  date_add(concat(substr('${key_3d}', 0, 4), '-', substr('${key_3d}', 5, 2), '-', substr('${key_3d}', 7, 2)),1)        AS statDate,\\n  d.tenantId,\\n  d.shopId,\\n  d.customerId,\\n  if(d.cnt > 0, 1, 0)            AS lable\\nFROM(\\nSELECT\\n  tenantId,\\n  min(shopId)                    AS shopId,\\n  customerId,\\n  count(1)                       AS cnt\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE (ds BETWEEN '${key_3d}' AND '${key_f3d}') AND isnotnull(customerId) \\nGROUP BY tenantId, customerId) d;\\n\\n-------- 前四天 ---------\\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_4d}')\\nSELECT\\n  date_add(concat(substr('${key_4d}', 0, 4), '-', substr('${key_4d}', 5, 2), '-', substr('${key_4d}', 7, 2)),1)        AS statDate,\\n  d.tenantId,\\n  d.shopId,\\n  d.customerId,\\n  if(d.cnt > 0, 1, 0)            AS lable\\nFROM(\\nSELECT\\n  tenantId,\\n  min(shopId)                    AS shopId,\\n  customerId,\\n  count(1)                       AS cnt\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE (ds BETWEEN '${key_4d}' AND '${key_f2d}') AND isnotnull(customerId) \\nGROUP BY tenantId, customerId) d;\\n\\n\\n-------- 前五天 ---------\\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_5d}')\\nSELECT\\n  date_add(concat(substr('${key_5d}', 0, 4), '-', substr('${key_5d}', 5, 2), '-', substr('${key_5d}', 7, 2)),1)        AS statDate,\\n  d.tenantId,\\n  d.shopId,\\n  d.customerId,\\n  if(d.cnt > 0, 1, 0)            AS lable\\nFROM(\\nSELECT\\n  tenantId,\\n  min(shopId)                    AS shopId,\\n  customerId,\\n  count(1)                       AS cnt\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE (ds BETWEEN '${key_5d}' AND '${key_f1d}') AND isnotnull(customerId) \\nGROUP BY tenantId, customerId) d;\\n\\n\\n-------- 前六天 ---------\\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_6d}')\\nSELECT\\n  date_add(concat(substr('${key_6d}', 0, 4), '-', substr('${key_6d}', 5, 2), '-', substr('${key_6d}', 7, 2)),1)        AS statDate,\\n  d.tenantId,\\n  d.shopId,\\n  d.customerId,\\n  if(d.cnt > 0, 1, 0)            AS lable\\nFROM(\\nSELECT\\n  tenantId,\\n  min(shopId)                    AS shopId,\\n  customerId,\\n  count(1)                       AS cnt\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE (ds BETWEEN '${key_6d}' AND substr('${bdp.system.cyctime}',0,8)) AND isnotnull(customerId) \\nGROUP BY tenantId, customerId) d;\\n\\n\\n-------- 前七天 ---------\\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_7d}')\\nSELECT\\n  date_add(concat(substr('${key_7d}', 0, 4), '-', substr('${key_7d}', 5, 2), '-', substr('${key_7d}', 7, 2)),1)        AS statDate,\\n  d.tenantId,\\n  d.shopId,\\n  d.customerId,\\n  if(d.cnt > 0, 1, 0)            AS lable\\nFROM(\\nSELECT\\n  tenantId,\\n  min(shopId)                    AS shopId,\\n  customerId,\\n  count(1)                       AS cnt\\nFROM dws_alg_trd_pay_cust_1d\\nWHERE (ds BETWEEN '${key_7d}' AND '${bdp.system.bizdate}') AND isnotnull(customerId) \\nGROUP BY tenantId, customerId) d;\\n\\n\",\"taskParams\":\"##Number of CPU cores need driver program is running\\n##driver.cores=1\\n\\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\\n##driver.maxResultSize=1g\\n\\n##Driver number memory used by a process\\n##driver.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##executor.memory=512m\\n\\n\\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\\n##logConf=spark.logConf\",\"scheduleConf\":\"{\\\"selfReliance\\\":false, \\\"min\\\":0,\\\"hour\\\":0,\\\"periodType\\\":\\\"2\\\",\\\"beginDate\\\":\\\"2001-01-01\\\",\\\"endDate\\\":\\\"2121-01-01\\\"}\",\"periodType\":null,\"scheduleStatus\":1,\"submitStatus\":0,\"modifyUserId\":90,\"createUserId\":90,\"ownerUserId\":90,\"version\":1,\"nodePid\":1193,\"taskDesc\":\"\",\"mainClass\":\"\",\"exeArgs\":\"\",\"flowId\":1050,\"createUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"modifyUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"ownerUser\":{\"id\":90,\"gmtCreate\":1535092930000,\"gmtModified\":1535092930000,\"isDeleted\":0,\"userName\":\"yizhou@dtstack.com\",\"phoneNumber\":null,\"dtuicUserId\":6999,\"email\":\"\",\"status\":0,\"defaultProjectId\":null},\"taskPeriodId\":2,\"taskPeriodType\":\"天任务\",\"nodePName\":\"工作流\",\"readWriteLockVO\":{\"id\":0,\"gmtCreate\":null,\"gmtModified\":1537327573000,\"isDeleted\":0,\"lockName\":null,\"modifyUserId\":null,\"version\":1,\"projectId\":null,\"relationId\":null,\"type\":null,\"lastKeepLockUserName\":\"yizhou@dtstack.com\",\"result\":0,\"getLock\":true},\"userId\":90,\"lockVersion\":null,\"taskVariables\":[{\"paramName\":\"bdp.system.bizdate\",\"paramCommand\":\"yyyyMMdd-1\",\"type\":0},{\"paramName\":\"key_f5d\",\"paramCommand\":\"$[yyyyMMdd+5]\",\"type\":1},{\"paramName\":\"key_2d\",\"paramCommand\":\"$[yyyyMMdd-2]\",\"type\":1},{\"paramName\":\"key_f4d\",\"paramCommand\":\"$[yyyyMMdd+4]\",\"type\":1},{\"paramName\":\"key_3d\",\"paramCommand\":\"$[yyyyMMdd-3]\",\"type\":1},{\"paramName\":\"key_f3d\",\"paramCommand\":\"$[yyyyMMdd+3]\",\"type\":1},{\"paramName\":\"key_4d\",\"paramCommand\":\"$[yyyyMMdd-4]\",\"type\":1},{\"paramName\":\"key_f2d\",\"paramCommand\":\"$[yyyyMMdd+2]\",\"type\":1},{\"paramName\":\"key_5d\",\"paramCommand\":\"$[yyyyMMdd-5]\",\"type\":1},{\"paramName\":\"key_f1d\",\"paramCommand\":\"$[yyyyMMdd+1]\",\"type\":1},{\"paramName\":\"key_6d\",\"paramCommand\":\"$[yyyyMMdd-6]\",\"type\":1},{\"paramName\":\"bdp.system.cyctime\",\"paramCommand\":\"yyyyMMddHHmmss\",\"type\":0},{\"paramName\":\"key_7d\",\"paramCommand\":\"$[yyyyMMdd-7]\",\"type\":1}],\"forceUpdate\":false,\"subNodes\":null,\"createModel\":0,\"operateModel\":0,\"pythonVersion\":0,\"learningType\":0,\"input\":null,\"output\":null,\"options\":null,\"flowName\":\"flow_job\",\"taskVOS\":null,\"subTaskVOS\":null,\"resourceList\":[],\"taskVersions\":[],\"cron\":\"0 0 0 * * ?\",\"merged\":false,\"cursorPosition\":{\"lineNumber\":144,\"column\":1}},\"x\":850,\"y\":80,\"value\":null,\"id\":\"1062\"}}]",
                    "taskParams": "##Number of CPU cores need driver program is running\n##driver.cores=1\n\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\n##driver.maxResultSize=1g\n\n##Driver number memory used by a process\n##driver.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##executor.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##logConf=spark.logConf",
                    "scheduleConf": "{\"selfReliance\":0,\"min\":\"0\",\"hour\":\"4\",\"periodType\":\"2\",\"beginDate\":\"2001-01-01\",\"endDate\":\"2121-01-01\"}",
                    "periodType": null,
                    "scheduleStatus": 2,
                    "submitStatus": 1,
                    "modifyUserId": 90,
                    "createUserId": 90,
                    "ownerUserId": 90,
                    "version": 16,
                    "nodePid": 1193,
                    "taskDesc": "",
                    "mainClass": "",
                    "exeArgs": "",
                    "flowId": 0,
                    "createUser": {
                        "id": 90,
                        "gmtCreate": 1535092930000,
                        "gmtModified": 1535092930000,
                        "isDeleted": 0,
                        "userName": "yizhou@dtstack.com",
                        "phoneNumber": null,
                        "dtuicUserId": 6999,
                        "email": "",
                        "status": 0,
                        "defaultProjectId": null
                    },
                    "modifyUser": null,
                    "ownerUser": null,
                    "taskPeriodId": 2,
                    "taskPeriodType": "天任务",
                    "nodePName": null,
                    "readWriteLockVO": null,
                    "userId": null,
                    "lockVersion": null,
                    "taskVariables": null,
                    "forceUpdate": false,
                    "subNodes": null,
                    "relatedTasks": null,
                    "createModel": 0,
                    "operateModel": 0,
                    "pythonVersion": 0,
                    "learningType": 0,
                    "input": null,
                    "output": null,
                    "options": null,
                    "flowName": null,
                    "syncModel": 0,
                    "increColumn": null,
                    "taskVOS": null,
                    "subTaskVOS": null,
                    "resourceList": null,
                    "refResourceList": null,
                    "taskVersions": null,
                    "cron": "0 0 4 * * ?"
                },
                "id": 278708,
                "gmtCreate": 1547647312000,
                "gmtModified": 1547647312000,
                "isDeleted": 0,
                "tenantId": null,
                "projectId": null,
                "jobId": "58a98fff",
                "jobKey": "cronTrigger_1050_20190117040000",
                "jobName": "cronJob_dws_alg_flow_job_20190117040000",
                "status": 18,
                "taskId": 1050,
                "createUserId": 90,
                "type": 0,
                "businessDate": "2019-01-16 ",
                "cycTime": "2019-01-17 04:00:00",
                "execStartTime": null,
                "execEndTime": null,
                "execTime": null,
                "execStartDate": null,
                "execEndDate": null,
                "taskPeriodId": 2,
                "taskPeriodType": "天任务",
                "jobVOS": null,
                "batchEngineJob": {
                    "id": 297206,
                    "gmtCreate": 1547647312000,
                    "gmtModified": 1547668804000,
                    "isDeleted": 0,
                    "status": 18,
                    "jobId": "58a98fff",
                    "engineJobId": null,
                    "logInfo": "{ \"msg_info\": \"当前任务处于冻结状态\"}",
                    "engineLog": "",
                    "execStartTime": null,
                    "execEndTime": null,
                    "execTime": 0
                },
                "subNodes": {
                    "batchTask": {
                        "id": 1051,
                        "gmtCreate": 1537327019000,
                        "gmtModified": 1537327772000,
                        "isDeleted": 0,
                        "tenantId": 129,
                        "projectId": 77,
                        "name": "job_dws_alg_1d",
                        "taskType": 0,
                        "computeType": 1,
                        "engineType": 1,
                        "sqlText": "CREATE TABLE if NOT EXISTS dws_alg_trd_pay_cust_1d(\nstatDate            STRING      COMMENT '统计日期'\n,tenantId           STRING      COMMENT '租户ID'\n,shopId             STRING      COMMENT '门店ID'\n,customerId         STRING      COMMENT '顾客ID'\n,max_payment_time   STRING      COMMENT '最近一天最晚支付时间'\n,inter_days_1d      BIGINT      COMMENT '最近一天顾客最后一次发生支付行为距离当前日期多少天'\n,pay_quantity_1d    BIGINT      COMMENT '最近一天顾客支付行为次数'\n,pay_amount_1d      DOUBLE      COMMENT '最近一天顾客支付总金额'\n,tot_days_1d        BIGINT      COMMENT '最近一天顾客出现支付行为的天数之和'\n) COMMENT '最近1天算法信息指标'\nPARTITIONED BY (ds STRING ) STORED AS orc LIFECYCLE 365;\n\nset spark.sql.shuffle.partitions=1;\nset hive.exec.dynamic.partition.mode=nonstrict;\n\n\nINSERT OVERWRITE TABLE dws_alg_trd_pay_cust_1d PARTITION(ds)\nSELECT\n  date_add(concat(substr(t.par, 0, 4), '-', substr(t.par, 5, 2), '-', substr(t.par, 7, 2)),1)        AS statDate,\n  t.tenantId              AS tenantId,\n  t.shopId                AS shopId,\n  t.customerId            AS customerId,\n  t.paymentTime           AS max_payment_time,\n  datediff(date_add(concat(substr(t.par, 0, 4), '-', substr(t.par, 5, 2), '-', substr(t.par, 7, 2)),1),\n           t.paymentTime) AS inter_days_1d,\n  t.pay_quantity_1d       AS pay_quantity_1d,\n  t.pay_amount_1d         AS pay_amount_1d,\n  t.tot_days_1d           AS tot_days_1d,\n  t.par                   AS ds\nFROM (\n       SELECT\n         tenantId,\n         min(shopId)                                                              AS shopId,\n         customerId,\n         max(paymentTime)                                                         AS paymentTime,\n         COUNT(1)                                                                 AS pay_quantity_1d,\n         sum(custActualPay)                                                       AS pay_amount_1d,\n         size(collect_set(substr(regexp_replace(paymentTime, '-', ''), 1, 8)))    AS tot_days_1d,\n         date_format(paymentTime, 'yyyyMMdd')                                     AS par\n       FROM dwd_ord_trd_d\n       WHERE ds = '${bdp.system.bizdate}' AND tradePayStatus = 3 AND isnotnull(customerId)\n       GROUP BY tenantId, customerId, date_format(paymentTime, 'yyyyMMdd')) t;\n",
                        "taskParams": "##Number of CPU cores need driver program is running\n##driver.cores=1\n\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\n##driver.maxResultSize=1g\n\n##Driver number memory used by a process\n##driver.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##executor.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##logConf=spark.logConf",
                        "scheduleConf": "{\"beginDate\":\"2001-01-01\",\"min\":\"0\",\"periodType\":\"2\",\"hour\":\"4\",\"selfReliance\":0,\"endDate\":\"2121-01-01\"}",
                        "periodType": null,
                        "scheduleStatus": 1,
                        "submitStatus": 1,
                        "modifyUserId": 90,
                        "createUserId": 90,
                        "ownerUserId": 90,
                        "version": 3,
                        "nodePid": 1193,
                        "taskDesc": "",
                        "mainClass": "",
                        "exeArgs": "",
                        "flowId": 1050,
                        "createUser": {
                            "id": 90,
                            "gmtCreate": 1535092930000,
                            "gmtModified": 1535092930000,
                            "isDeleted": 0,
                            "userName": "yizhou@dtstack.com",
                            "phoneNumber": null,
                            "dtuicUserId": 6999,
                            "email": "",
                            "status": 0,
                            "defaultProjectId": null
                        },
                        "modifyUser": null,
                        "ownerUser": null,
                        "taskPeriodId": 2,
                        "taskPeriodType": "天任务",
                        "nodePName": null,
                        "readWriteLockVO": null,
                        "userId": null,
                        "lockVersion": null,
                        "taskVariables": null,
                        "forceUpdate": false,
                        "subNodes": null,
                        "relatedTasks": null,
                        "createModel": 0,
                        "operateModel": 0,
                        "pythonVersion": 0,
                        "learningType": 0,
                        "input": null,
                        "output": null,
                        "options": null,
                        "flowName": null,
                        "syncModel": 0,
                        "increColumn": null,
                        "taskVOS": null,
                        "subTaskVOS": null,
                        "resourceList": null,
                        "refResourceList": null,
                        "taskVersions": null,
                        "cron": "0 0 4 * * ?"
                    },
                    "id": 278709,
                    "gmtCreate": 1547647312000,
                    "gmtModified": 1547647312000,
                    "isDeleted": 0,
                    "tenantId": null,
                    "projectId": null,
                    "jobId": "1cb14eb7",
                    "jobKey": "cronTrigger_1051_20190117040000",
                    "jobName": "cronJob_job_dws_alg_1d_20190117040000",
                    "status": 18,
                    "taskId": 1051,
                    "createUserId": 90,
                    "type": 0,
                    "businessDate": "2019-01-16 ",
                    "cycTime": "2019-01-17 04:00:00",
                    "execStartTime": null,
                    "execEndTime": null,
                    "execTime": null,
                    "execStartDate": null,
                    "execEndDate": null,
                    "taskPeriodId": 2,
                    "taskPeriodType": "天任务",
                    "jobVOS": [
                        {
                            "batchTask": {
                                "id": 1052,
                                "gmtCreate": 1537327160000,
                                "gmtModified": 1537327772000,
                                "isDeleted": 0,
                                "tenantId": 129,
                                "projectId": 77,
                                "name": "job_dws_alg_3d",
                                "taskType": 0,
                                "computeType": 1,
                                "engineType": 1,
                                "sqlText": "CREATE TABLE if NOT EXISTS dws_alg_trd_pay_cust_3d(\nstatDate            STRING      COMMENT '统计日期'\n,tenantId           STRING      COMMENT '租户ID'\n,shopId             STRING      COMMENT '门店ID'\n,customerId         STRING      COMMENT '顾客ID'\n,inter_days_3d      BIGINT      COMMENT '最近3天顾客最后一次发生支付行为距离当前日期多少天'\n,pay_quantity_3d    BIGINT      COMMENT '最近3天顾客支付行为次数'\n,pay_amount_3d      DOUBLE      COMMENT '最近3天顾客支付总金额'\n,tot_days_3d        BIGINT      COMMENT '最近3天顾客出现支付行为的天数之和'\n) COMMENT '最近3天算法信息指标'\nPARTITIONED BY (ds STRING ) STORED AS orc LIFECYCLE 365;\n\nset spark.sql.shuffle.partitions=1;\n\nINSERT OVERWRITE TABLE dws_alg_trd_pay_cust_3d PARTITION(ds = '${bdp.system.bizdate}')\nSELECT\n  date_add(concat(\n               substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\n               substr('${bdp.system.bizdate}', 7, 2)), 1) AS statDate,\n  tenantId,\n  min(shopId)                                             AS shopId,\n  customerId,\n  datediff(date_add(concat(substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\n                           substr('${bdp.system.bizdate}', 7, 2)), 1),\n           max(max_payment_time))                         AS inter_days_3d,\n  sum(pay_quantity_1d)                                    AS pay_quantity_3d,\n  sum(pay_amount_1d)                                      AS pay_amount_3d,\n  sum(tot_days_1d)                                        AS tot_days_3d\nFROM dws_alg_trd_pay_cust_1d\nWHERE ds BETWEEN '${key_3d}' AND '${bdp.system.bizdate}'\nGROUP BY tenantId, customerId;\n",
                                "taskParams": "##Number of CPU cores need driver program is running\n##driver.cores=1\n\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\n##driver.maxResultSize=1g\n\n##Driver number memory used by a process\n##driver.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##executor.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##logConf=spark.logConf",
                                "scheduleConf": "{\"beginDate\":\"2001-01-01\",\"min\":\"0\",\"periodType\":\"2\",\"hour\":\"4\",\"selfReliance\":0,\"endDate\":\"2121-01-01\"}",
                                "periodType": null,
                                "scheduleStatus": 1,
                                "submitStatus": 1,
                                "modifyUserId": 90,
                                "createUserId": 90,
                                "ownerUserId": 90,
                                "version": 3,
                                "nodePid": 1193,
                                "taskDesc": "",
                                "mainClass": "",
                                "exeArgs": "",
                                "flowId": 1050,
                                "createUser": {
                                    "id": 90,
                                    "gmtCreate": 1535092930000,
                                    "gmtModified": 1535092930000,
                                    "isDeleted": 0,
                                    "userName": "yizhou@dtstack.com",
                                    "phoneNumber": null,
                                    "dtuicUserId": 6999,
                                    "email": "",
                                    "status": 0,
                                    "defaultProjectId": null
                                },
                                "modifyUser": null,
                                "ownerUser": null,
                                "taskPeriodId": 2,
                                "taskPeriodType": "天任务",
                                "nodePName": null,
                                "readWriteLockVO": null,
                                "userId": null,
                                "lockVersion": null,
                                "taskVariables": null,
                                "forceUpdate": false,
                                "subNodes": null,
                                "relatedTasks": null,
                                "createModel": 0,
                                "operateModel": 0,
                                "pythonVersion": 0,
                                "learningType": 0,
                                "input": null,
                                "output": null,
                                "options": null,
                                "flowName": null,
                                "syncModel": 0,
                                "increColumn": null,
                                "taskVOS": null,
                                "subTaskVOS": null,
                                "resourceList": null,
                                "refResourceList": null,
                                "taskVersions": null,
                                "cron": "0 0 4 * * ?"
                            },
                            "id": 278710,
                            "gmtCreate": 1547647312000,
                            "gmtModified": 1547647312000,
                            "isDeleted": 0,
                            "tenantId": null,
                            "projectId": null,
                            "jobId": "d5187e4a",
                            "jobKey": "cronTrigger_1052_20190117040000",
                            "jobName": "cronJob_job_dws_alg_3d_20190117040000",
                            "status": 18,
                            "taskId": 1052,
                            "createUserId": 90,
                            "type": 0,
                            "businessDate": "2019-01-16 ",
                            "cycTime": "2019-01-17 04:00:00",
                            "execStartTime": null,
                            "execEndTime": null,
                            "execTime": null,
                            "execStartDate": null,
                            "execEndDate": null,
                            "taskPeriodId": 2,
                            "taskPeriodType": "天任务",
                            "jobVOS": null,
                            "batchEngineJob": {
                                "id": 297208,
                                "gmtCreate": 1547647312000,
                                "gmtModified": 1547668804000,
                                "isDeleted": 0,
                                "status": 18,
                                "jobId": "d5187e4a",
                                "engineJobId": null,
                                "logInfo": "{ \"msg_info\": \"依赖任务链路存在任务处于冻结状态\"}",
                                "engineLog": "",
                                "execStartTime": null,
                                "execEndTime": null,
                                "execTime": 0
                            },
                            "subNodes": null,
                            "flowJobId": "58a98fff",
                            "relatedJobs": null,
                            "isDirty": 0
                        },
                        {
                            "batchTask": {
                                "id": 1053,
                                "gmtCreate": 1537327261000,
                                "gmtModified": 1537327772000,
                                "isDeleted": 0,
                                "tenantId": 129,
                                "projectId": 77,
                                "name": "job_dws_alg_7d",
                                "taskType": 0,
                                "computeType": 1,
                                "engineType": 1,
                                "sqlText": "CREATE TABLE if NOT EXISTS dws_alg_trd_pay_cust_7d(\nstatDate            STRING      COMMENT '统计日期'\n,tenantId           STRING      COMMENT '租户ID'\n,shopId             STRING      COMMENT '门店ID'\n,customerId         STRING      COMMENT '顾客ID'\n,inter_days_7d      BIGINT      COMMENT '最近7天顾客最后一次发生支付行为距离当前日期多少天'\n,pay_quantity_7d    BIGINT      COMMENT '最近7天顾客支付行为次数'\n,pay_amount_7d      DOUBLE      COMMENT '最近7天顾客支付总金额'\n,tot_days_7d        BIGINT      COMMENT '最近7天顾客出现支付行为的天数之和'\n) COMMENT '最近7天算法信息指标'\nPARTITIONED BY (ds STRING ) STORED AS orc LIFECYCLE 365;\n\nset spark.sql.shuffle.partitions=1;\n\nINSERT OVERWRITE TABLE dws_alg_trd_pay_cust_7d PARTITION(ds = '${bdp.system.bizdate}')\nSELECT\n  date_add(concat(\n               substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\n               substr('${bdp.system.bizdate}', 7, 2)), 1) AS statDate,\n  tenantId,\n  min(shopId)                                             AS shopId,\n  customerId,\n  datediff(date_add(concat(substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\n                           substr('${bdp.system.bizdate}', 7, 2)), 1),\n           max(max_payment_time))                         AS inter_days_7d,\n  sum(pay_quantity_1d)                                    AS pay_quantity_7d,\n  sum(pay_amount_1d)                                      AS pay_amount_7d,\n  sum(tot_days_1d)                                        AS tot_days_7d\nFROM dws_alg_trd_pay_cust_1d\nWHERE ds BETWEEN '${key_7d}' AND '${bdp.system.bizdate}'\nGROUP BY tenantId, customerId;\n",
                                "taskParams": "##Number of CPU cores need driver program is running\n##driver.cores=1\n\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\n##driver.maxResultSize=1g\n\n##Driver number memory used by a process\n##driver.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##executor.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##logConf=spark.logConf",
                                "scheduleConf": "{\"beginDate\":\"2001-01-01\",\"min\":\"0\",\"periodType\":\"2\",\"hour\":\"4\",\"selfReliance\":0,\"endDate\":\"2121-01-01\"}",
                                "periodType": null,
                                "scheduleStatus": 1,
                                "submitStatus": 1,
                                "modifyUserId": 90,
                                "createUserId": 90,
                                "ownerUserId": 90,
                                "version": 4,
                                "nodePid": 1193,
                                "taskDesc": "",
                                "mainClass": "",
                                "exeArgs": "",
                                "flowId": 1050,
                                "createUser": {
                                    "id": 90,
                                    "gmtCreate": 1535092930000,
                                    "gmtModified": 1535092930000,
                                    "isDeleted": 0,
                                    "userName": "yizhou@dtstack.com",
                                    "phoneNumber": null,
                                    "dtuicUserId": 6999,
                                    "email": "",
                                    "status": 0,
                                    "defaultProjectId": null
                                },
                                "modifyUser": null,
                                "ownerUser": null,
                                "taskPeriodId": 2,
                                "taskPeriodType": "天任务",
                                "nodePName": null,
                                "readWriteLockVO": null,
                                "userId": null,
                                "lockVersion": null,
                                "taskVariables": null,
                                "forceUpdate": false,
                                "subNodes": null,
                                "relatedTasks": null,
                                "createModel": 0,
                                "operateModel": 0,
                                "pythonVersion": 0,
                                "learningType": 0,
                                "input": null,
                                "output": null,
                                "options": null,
                                "flowName": null,
                                "syncModel": 0,
                                "increColumn": null,
                                "taskVOS": null,
                                "subTaskVOS": null,
                                "resourceList": null,
                                "refResourceList": null,
                                "taskVersions": null,
                                "cron": "0 0 4 * * ?"
                            },
                            "id": 278711,
                            "gmtCreate": 1547647312000,
                            "gmtModified": 1547647312000,
                            "isDeleted": 0,
                            "tenantId": null,
                            "projectId": null,
                            "jobId": "d715e37a",
                            "jobKey": "cronTrigger_1053_20190117040000",
                            "jobName": "cronJob_job_dws_alg_7d_20190117040000",
                            "status": 18,
                            "taskId": 1053,
                            "createUserId": 90,
                            "type": 0,
                            "businessDate": "2019-01-16 ",
                            "cycTime": "2019-01-17 04:00:00",
                            "execStartTime": null,
                            "execEndTime": null,
                            "execTime": null,
                            "execStartDate": null,
                            "execEndDate": null,
                            "taskPeriodId": 2,
                            "taskPeriodType": "天任务",
                            "jobVOS": null,
                            "batchEngineJob": {
                                "id": 297209,
                                "gmtCreate": 1547647312000,
                                "gmtModified": 1547668804000,
                                "isDeleted": 0,
                                "status": 18,
                                "jobId": "d715e37a",
                                "engineJobId": null,
                                "logInfo": "{ \"msg_info\": \"依赖任务链路存在任务处于冻结状态\"}",
                                "engineLog": "",
                                "execStartTime": null,
                                "execEndTime": null,
                                "execTime": 0
                            },
                            "subNodes": null,
                            "flowJobId": "58a98fff",
                            "relatedJobs": null,
                            "isDirty": 0
                        },
                        {
                            "batchTask": {
                                "id": 1054,
                                "gmtCreate": 1537327327000,
                                "gmtModified": 1537327772000,
                                "isDeleted": 0,
                                "tenantId": 129,
                                "projectId": 77,
                                "name": "job_dws_alg_15d",
                                "taskType": 0,
                                "computeType": 1,
                                "engineType": 1,
                                "sqlText": "CREATE TABLE if NOT EXISTS dws_alg_trd_pay_cust_15d(\nstatDate             STRING      COMMENT '统计日期'\n,tenantId            STRING      COMMENT '租户ID'\n,shopId              STRING      COMMENT '门店ID'\n,customerId          STRING      COMMENT '顾客ID'\n,inter_days_15d      BIGINT      COMMENT '最近15天顾客最后一次发生支付行为距离当前日期多少天'\n,pay_quantity_15d    BIGINT      COMMENT '最近15天顾客支付行为次数'\n,pay_amount_15d      DOUBLE      COMMENT '最近15天顾客支付总金额'\n,tot_days_15d        BIGINT      COMMENT '最近15天顾客出现支付行为的天数之和'\n) COMMENT '最近15天算法信息指标'\nPARTITIONED BY (ds STRING );\n\nset spark.sql.shuffle.partitions=1;\n\nINSERT OVERWRITE TABLE dws_alg_trd_pay_cust_15d PARTITION(ds = '${bdp.system.bizdate}')\nSELECT\n  date_add(concat(\n               substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\n               substr('${bdp.system.bizdate}', 7, 2)), 1) AS statDate,\n  tenantId,\n  min(shopId)                                             AS shopId,\n  customerId,\n  datediff(date_add(concat(substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\n                           substr('${bdp.system.bizdate}', 7, 2)), 1),\n           max(max_payment_time))                         AS inter_days_15d,\n  sum(pay_quantity_1d)                                    AS pay_quantity_15d,\n  sum(pay_amount_1d)                                      AS pay_amount_15d,\n  sum(tot_days_1d)                                        AS tot_days_15d\nFROM dws_alg_trd_pay_cust_1d\nWHERE ds BETWEEN '${key_15d}' AND '${bdp.system.bizdate}'\nGROUP BY tenantId, customerId;",
                                "taskParams": "##Number of CPU cores need driver program is running\n##driver.cores=1\n\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\n##driver.maxResultSize=1g\n\n##Driver number memory used by a process\n##driver.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##executor.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##logConf=spark.logConf",
                                "scheduleConf": "{\"beginDate\":\"2001-01-01\",\"min\":\"0\",\"periodType\":\"2\",\"hour\":\"4\",\"selfReliance\":0,\"endDate\":\"2121-01-01\"}",
                                "periodType": null,
                                "scheduleStatus": 1,
                                "submitStatus": 1,
                                "modifyUserId": 90,
                                "createUserId": 90,
                                "ownerUserId": 90,
                                "version": 2,
                                "nodePid": 1193,
                                "taskDesc": "",
                                "mainClass": "",
                                "exeArgs": "",
                                "flowId": 1050,
                                "createUser": {
                                    "id": 90,
                                    "gmtCreate": 1535092930000,
                                    "gmtModified": 1535092930000,
                                    "isDeleted": 0,
                                    "userName": "yizhou@dtstack.com",
                                    "phoneNumber": null,
                                    "dtuicUserId": 6999,
                                    "email": "",
                                    "status": 0,
                                    "defaultProjectId": null
                                },
                                "modifyUser": null,
                                "ownerUser": null,
                                "taskPeriodId": 2,
                                "taskPeriodType": "天任务",
                                "nodePName": null,
                                "readWriteLockVO": null,
                                "userId": null,
                                "lockVersion": null,
                                "taskVariables": null,
                                "forceUpdate": false,
                                "subNodes": null,
                                "relatedTasks": null,
                                "createModel": 0,
                                "operateModel": 0,
                                "pythonVersion": 0,
                                "learningType": 0,
                                "input": null,
                                "output": null,
                                "options": null,
                                "flowName": null,
                                "syncModel": 0,
                                "increColumn": null,
                                "taskVOS": null,
                                "subTaskVOS": null,
                                "resourceList": null,
                                "refResourceList": null,
                                "taskVersions": null,
                                "cron": "0 0 4 * * ?"
                            },
                            "id": 278712,
                            "gmtCreate": 1547647312000,
                            "gmtModified": 1547647312000,
                            "isDeleted": 0,
                            "tenantId": null,
                            "projectId": null,
                            "jobId": "2a0926c2",
                            "jobKey": "cronTrigger_1054_20190117040000",
                            "jobName": "cronJob_job_dws_alg_15d_20190117040000",
                            "status": 18,
                            "taskId": 1054,
                            "createUserId": 90,
                            "type": 0,
                            "businessDate": "2019-01-16 ",
                            "cycTime": "2019-01-17 04:00:00",
                            "execStartTime": null,
                            "execEndTime": null,
                            "execTime": null,
                            "execStartDate": null,
                            "execEndDate": null,
                            "taskPeriodId": 2,
                            "taskPeriodType": "天任务",
                            "jobVOS": null,
                            "batchEngineJob": {
                                "id": 297210,
                                "gmtCreate": 1547647312000,
                                "gmtModified": 1547668804000,
                                "isDeleted": 0,
                                "status": 18,
                                "jobId": "2a0926c2",
                                "engineJobId": null,
                                "logInfo": "{ \"msg_info\": \"依赖任务链路存在任务处于冻结状态\"}",
                                "engineLog": "",
                                "execStartTime": null,
                                "execEndTime": null,
                                "execTime": 0
                            },
                            "subNodes": null,
                            "flowJobId": "58a98fff",
                            "relatedJobs": null,
                            "isDirty": 0
                        },
                        {
                            "batchTask": {
                                "id": 1055,
                                "gmtCreate": 1537327392000,
                                "gmtModified": 1537327772000,
                                "isDeleted": 0,
                                "tenantId": 129,
                                "projectId": 77,
                                "name": "job_dws_alg_30d",
                                "taskType": 0,
                                "computeType": 1,
                                "engineType": 1,
                                "sqlText": "CREATE TABLE if NOT EXISTS dws_alg_trd_pay_cust_30d(\nstatDate                STRING      COMMENT '统计日期'\n,tenantId               STRING      COMMENT '租户ID'\n,shopId                 STRING      COMMENT '门店ID'\n,customerId             STRING      COMMENT '顾客ID'\n,inter_days_30d         BIGINT      COMMENT '最近30天顾客最后一次发生支付行为距离当前日期多少天'\n,pay_quantity_30d       BIGINT      COMMENT '最近30天顾客支付行为次数'\n,pay_amount_30d         DOUBLE      COMMENT '最近30天顾客支付总金额'\n,tot_days_30d           BIGINT      COMMENT '最近30天顾客出现支付行为的天数之和'\n) COMMENT '最近30天算法信息指标'\nPARTITIONED BY (ds STRING ) STORED AS orc LIFECYCLE 365;\n\nset spark.sql.shuffle.partitions=1;\n\nINSERT OVERWRITE TABLE dws_alg_trd_pay_cust_30d PARTITION(ds = '${bdp.system.bizdate}')\nSELECT\n  date_add(concat(\n               substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\n               substr('${bdp.system.bizdate}', 7, 2)), 1) AS statDate,\n  tenantId,\n  min(shopId)                                             AS shopId,\n  customerId,\n  datediff(date_add(concat(substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\n                           substr('${bdp.system.bizdate}', 7, 2)), 1),\n           max(max_payment_time))                         AS inter_days_30d,\n  sum(pay_quantity_1d)                                    AS pay_quantity_30d,\n  sum(pay_amount_1d)                                      AS pay_amount_30d,\n  sum(tot_days_1d)                                        AS tot_days_30d\nFROM dws_alg_trd_pay_cust_1d\nWHERE ds BETWEEN '${key_30d}' AND '${bdp.system.bizdate}'\nGROUP BY tenantId, customerId;",
                                "taskParams": "##Number of CPU cores need driver program is running\n##driver.cores=1\n\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\n##driver.maxResultSize=1g\n\n##Driver number memory used by a process\n##driver.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##executor.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##logConf=spark.logConf",
                                "scheduleConf": "{\"beginDate\":\"2001-01-01\",\"min\":\"0\",\"periodType\":\"2\",\"hour\":\"4\",\"selfReliance\":0,\"endDate\":\"2121-01-01\"}",
                                "periodType": null,
                                "scheduleStatus": 1,
                                "submitStatus": 1,
                                "modifyUserId": 90,
                                "createUserId": 90,
                                "ownerUserId": 90,
                                "version": 2,
                                "nodePid": 1193,
                                "taskDesc": "",
                                "mainClass": "",
                                "exeArgs": "",
                                "flowId": 1050,
                                "createUser": {
                                    "id": 90,
                                    "gmtCreate": 1535092930000,
                                    "gmtModified": 1535092930000,
                                    "isDeleted": 0,
                                    "userName": "yizhou@dtstack.com",
                                    "phoneNumber": null,
                                    "dtuicUserId": 6999,
                                    "email": "",
                                    "status": 0,
                                    "defaultProjectId": null
                                },
                                "modifyUser": null,
                                "ownerUser": null,
                                "taskPeriodId": 2,
                                "taskPeriodType": "天任务",
                                "nodePName": null,
                                "readWriteLockVO": null,
                                "userId": null,
                                "lockVersion": null,
                                "taskVariables": null,
                                "forceUpdate": false,
                                "subNodes": null,
                                "relatedTasks": null,
                                "createModel": 0,
                                "operateModel": 0,
                                "pythonVersion": 0,
                                "learningType": 0,
                                "input": null,
                                "output": null,
                                "options": null,
                                "flowName": null,
                                "syncModel": 0,
                                "increColumn": null,
                                "taskVOS": null,
                                "subTaskVOS": null,
                                "resourceList": null,
                                "refResourceList": null,
                                "taskVersions": null,
                                "cron": "0 0 4 * * ?"
                            },
                            "id": 278713,
                            "gmtCreate": 1547647312000,
                            "gmtModified": 1547647312000,
                            "isDeleted": 0,
                            "tenantId": null,
                            "projectId": null,
                            "jobId": "e013cfee",
                            "jobKey": "cronTrigger_1055_20190117040000",
                            "jobName": "cronJob_job_dws_alg_30d_20190117040000",
                            "status": 18,
                            "taskId": 1055,
                            "createUserId": 90,
                            "type": 0,
                            "businessDate": "2019-01-16 ",
                            "cycTime": "2019-01-17 04:00:00",
                            "execStartTime": null,
                            "execEndTime": null,
                            "execTime": null,
                            "execStartDate": null,
                            "execEndDate": null,
                            "taskPeriodId": 2,
                            "taskPeriodType": "天任务",
                            "jobVOS": null,
                            "batchEngineJob": {
                                "id": 297211,
                                "gmtCreate": 1547647312000,
                                "gmtModified": 1547668804000,
                                "isDeleted": 0,
                                "status": 18,
                                "jobId": "e013cfee",
                                "engineJobId": null,
                                "logInfo": "{ \"msg_info\": \"依赖任务链路存在任务处于冻结状态\"}",
                                "engineLog": "",
                                "execStartTime": null,
                                "execEndTime": null,
                                "execTime": 0
                            },
                            "subNodes": null,
                            "flowJobId": "58a98fff",
                            "relatedJobs": null,
                            "isDirty": 0
                        },
                        {
                            "batchTask": {
                                "id": 1056,
                                "gmtCreate": 1537327469000,
                                "gmtModified": 1537327773000,
                                "isDeleted": 0,
                                "tenantId": 129,
                                "projectId": 77,
                                "name": "job_dws_alg_std",
                                "taskType": 0,
                                "computeType": 1,
                                "engineType": 1,
                                "sqlText": "CREATE TABLE if NOT EXISTS dws_alg_trd_pay_cust_std(\nstatDate                STRING      COMMENT '统计日期'\n,tenantId               STRING      COMMENT '租户ID'\n,shopId                 STRING      COMMENT '门店ID'\n,customerId             STRING      COMMENT '顾客ID'\n,inter_days_std         BIGINT      COMMENT '历史至今顾客最后一次发生支付行为距离当前日期多少天'\n,pay_quantity_std       BIGINT      COMMENT '历史至今顾客支付行为次数'\n,pay_amount_std         DOUBLE      COMMENT '历史至今顾客支付总金额'\n,tot_days_std           BIGINT      COMMENT '历史至今顾客出现支付行为的天数之和'\n) COMMENT '历史至今算法信息指标'\nPARTITIONED BY (ds STRING ) STORED AS orc LIFECYCLE 365;\n\nset spark.sql.shuffle.partitions=1;\n\n-- INSERT OVERWRITE TABLE dws_alg_trd_pay_cust_std PARTITION(ds = '${bdp.system.bizdate}')\n-- SELECT\n--   date_add(concat(substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-', substr('${bdp.system.bizdate}', 7, 2)),1)                                AS statDate,\n--   coalesce(d.tenantId, std.tenantId)                                 AS tenantId,\n--   coalesce(d.shopId, std.shopId)                                     AS shopId,\n--   coalesce(d.customerId, std.customerId)                             AS customerId,\n--   if(coalesce(d.inter_days_1d,0) = 1, 1, std.inter_days_std + 1)     AS inter_days_std,\n--   coalesce(d.pay_quantity_1d, 0) + coalesce(std.pay_quantity_std, 0) AS pay_quantity_std,\n--   coalesce(d.pay_amount_1d, 0) + coalesce(std.pay_amount_std, 0)     AS pay_amount_std,\n--   coalesce(d.tot_days_1d, 0) + coalesce(std.tot_days_std, 0)         AS tot_days_std\n-- FROM (\n--   SELECT \n--   tenantId,\n--   shopId,\n--   customerId,\n--   inter_days_1d,\n--   pay_quantity_1d,\n--   pay_amount_1d,\n--   tot_days_1d\n--   FROM \n--   dws_alg_trd_pay_cust_1d WHERE ds = '${bdp.system.bizdate}') d\n-- FULL OUTER JOIN (\n--   SELECT \n--   tenantId,\n--   shopId,\n--   customerId,\n--   inter_days_std,\n--   pay_quantity_std,\n--   pay_amount_std,\n--   tot_days_std \n--   FROM dws_alg_trd_pay_cust_std WHERE ds = '${key_2d}') std\n-- ON d.tenantId = std.tenantId AND d.customerId = std.customerId;\n\n\nINSERT OVERWRITE TABLE dws_alg_trd_pay_cust_std PARTITION(ds = '${bdp.system.bizdate}')\nSELECT\n  date_add(concat(\n               substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\n               substr('${bdp.system.bizdate}', 7, 2)), 1) AS statDate,\n  tenantId,\n  min(shopId)                                             AS shopId,\n  customerId,\n  datediff(date_add(concat(substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-',\n                           substr('${bdp.system.bizdate}', 7, 2)), 1),\n           max(max_payment_time))                         AS inter_days_std,\n  sum(pay_quantity_1d)                                    AS pay_quantity_std,\n  sum(pay_amount_1d)                                      AS pay_amount_std,\n  sum(tot_days_1d)                                        AS tot_days_std\nFROM dws_alg_trd_pay_cust_1d\nWHERE ds <= '${bdp.system.bizdate}'\nGROUP BY tenantId, customerId;\n",
                                "taskParams": "##Number of CPU cores need driver program is running\n##driver.cores=1\n\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\n##driver.maxResultSize=1g\n\n##Driver number memory used by a process\n##driver.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##executor.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##logConf=spark.logConf",
                                "scheduleConf": "{\"beginDate\":\"2001-01-01\",\"min\":\"0\",\"periodType\":\"2\",\"hour\":\"4\",\"selfReliance\":0,\"endDate\":\"2121-01-01\"}",
                                "periodType": null,
                                "scheduleStatus": 1,
                                "submitStatus": 1,
                                "modifyUserId": 90,
                                "createUserId": 90,
                                "ownerUserId": 90,
                                "version": 2,
                                "nodePid": 1193,
                                "taskDesc": "",
                                "mainClass": "",
                                "exeArgs": "",
                                "flowId": 1050,
                                "createUser": {
                                    "id": 90,
                                    "gmtCreate": 1535092930000,
                                    "gmtModified": 1535092930000,
                                    "isDeleted": 0,
                                    "userName": "yizhou@dtstack.com",
                                    "phoneNumber": null,
                                    "dtuicUserId": 6999,
                                    "email": "",
                                    "status": 0,
                                    "defaultProjectId": null
                                },
                                "modifyUser": null,
                                "ownerUser": null,
                                "taskPeriodId": 2,
                                "taskPeriodType": "天任务",
                                "nodePName": null,
                                "readWriteLockVO": null,
                                "userId": null,
                                "lockVersion": null,
                                "taskVariables": null,
                                "forceUpdate": false,
                                "subNodes": null,
                                "relatedTasks": null,
                                "createModel": 0,
                                "operateModel": 0,
                                "pythonVersion": 0,
                                "learningType": 0,
                                "input": null,
                                "output": null,
                                "options": null,
                                "flowName": null,
                                "syncModel": 0,
                                "increColumn": null,
                                "taskVOS": null,
                                "subTaskVOS": null,
                                "resourceList": null,
                                "refResourceList": null,
                                "taskVersions": null,
                                "cron": "0 0 4 * * ?"
                            },
                            "id": 278714,
                            "gmtCreate": 1547647312000,
                            "gmtModified": 1547647312000,
                            "isDeleted": 0,
                            "tenantId": null,
                            "projectId": null,
                            "jobId": "4fcf4094",
                            "jobKey": "cronTrigger_1056_20190117040000",
                            "jobName": "cronJob_job_dws_alg_std_20190117040000",
                            "status": 18,
                            "taskId": 1056,
                            "createUserId": 90,
                            "type": 0,
                            "businessDate": "2019-01-16 ",
                            "cycTime": "2019-01-17 04:00:00",
                            "execStartTime": null,
                            "execEndTime": null,
                            "execTime": null,
                            "execStartDate": null,
                            "execEndDate": null,
                            "taskPeriodId": 2,
                            "taskPeriodType": "天任务",
                            "jobVOS": null,
                            "batchEngineJob": {
                                "id": 297212,
                                "gmtCreate": 1547647312000,
                                "gmtModified": 1547668804000,
                                "isDeleted": 0,
                                "status": 18,
                                "jobId": "4fcf4094",
                                "engineJobId": null,
                                "logInfo": "{ \"msg_info\": \"依赖任务链路存在任务处于冻结状态\"}",
                                "engineLog": "",
                                "execStartTime": null,
                                "execEndTime": null,
                                "execTime": 0
                            },
                            "subNodes": null,
                            "flowJobId": "58a98fff",
                            "relatedJobs": null,
                            "isDirty": 0
                        },
                        {
                            "batchTask": {
                                "id": 1057,
                                "gmtCreate": 1537327573000,
                                "gmtModified": 1537327773000,
                                "isDeleted": 0,
                                "tenantId": 129,
                                "projectId": 77,
                                "name": "job_dws_alg_label",
                                "taskType": 0,
                                "computeType": 1,
                                "engineType": 1,
                                "sqlText": "CREATE TABLE if NOT EXISTS dws_alg_cust_label(\nstatDate                STRING      COMMENT '统计日期'\n,tenantId               STRING      COMMENT '租户ID'\n,shopId                 STRING      COMMENT '门店ID'\n,customerId             STRING      COMMENT '顾客ID'\n,label                  BIGINT      COMMENT '顾客未来七天内是否消费：1-是，0-否 (距离今日不足七天算最近区间)'\n) COMMENT '当日顾客未来七天内是否消费标记'\nPARTITIONED BY (ds STRING ) STORED AS orc LIFECYCLE 365;\n\n\nset spark.sql.shuffle.partitions=1;\n\n-------- 前一天 ----------\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${bdp.system.bizdate}')\nSELECT\n  date_add(concat(substr('${bdp.system.bizdate}', 0, 4), '-', substr('${bdp.system.bizdate}', 5, 2), '-', substr('${bdp.system.bizdate}', 7, 2)),1)        AS statDate,\n  d.tenantId,\n  d.shopId,\n  d.customerId,\n  if(d.cnt > 0, 1, 0)            AS lable\nFROM(\nSELECT\n  tenantId,\n  min(shopId)                    AS shopId,\n  customerId,\n  count(1)                       AS cnt\nFROM dws_alg_trd_pay_cust_1d\nWHERE (ds BETWEEN '${bdp.system.bizdate}' AND '${key_f5d}') AND isnotnull(customerId) \nGROUP BY tenantId, customerId) d;\n\n\n-------- 前两天 ---------\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_2d}')\nSELECT\n  date_add(concat(substr('${key_2d}', 0, 4), '-', substr('${key_2d}', 5, 2), '-', substr('${key_2d}', 7, 2)),1)        AS statDate,\n  d.tenantId,\n  d.shopId,\n  d.customerId,\n  if(d.cnt > 0, 1, 0)            AS lable\nFROM(\nSELECT\n  tenantId,\n  min(shopId)                    AS shopId,\n  customerId,\n  count(1)                       AS cnt\nFROM dws_alg_trd_pay_cust_1d\nWHERE (ds BETWEEN '${key_2d}' AND '${key_f4d}') AND isnotnull(customerId) \nGROUP BY tenantId, customerId) d;\n\n\n-------- 前三天 ---------\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_3d}')\nSELECT\n  date_add(concat(substr('${key_3d}', 0, 4), '-', substr('${key_3d}', 5, 2), '-', substr('${key_3d}', 7, 2)),1)        AS statDate,\n  d.tenantId,\n  d.shopId,\n  d.customerId,\n  if(d.cnt > 0, 1, 0)            AS lable\nFROM(\nSELECT\n  tenantId,\n  min(shopId)                    AS shopId,\n  customerId,\n  count(1)                       AS cnt\nFROM dws_alg_trd_pay_cust_1d\nWHERE (ds BETWEEN '${key_3d}' AND '${key_f3d}') AND isnotnull(customerId) \nGROUP BY tenantId, customerId) d;\n\n-------- 前四天 ---------\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_4d}')\nSELECT\n  date_add(concat(substr('${key_4d}', 0, 4), '-', substr('${key_4d}', 5, 2), '-', substr('${key_4d}', 7, 2)),1)        AS statDate,\n  d.tenantId,\n  d.shopId,\n  d.customerId,\n  if(d.cnt > 0, 1, 0)            AS lable\nFROM(\nSELECT\n  tenantId,\n  min(shopId)                    AS shopId,\n  customerId,\n  count(1)                       AS cnt\nFROM dws_alg_trd_pay_cust_1d\nWHERE (ds BETWEEN '${key_4d}' AND '${key_f2d}') AND isnotnull(customerId) \nGROUP BY tenantId, customerId) d;\n\n\n-------- 前五天 ---------\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_5d}')\nSELECT\n  date_add(concat(substr('${key_5d}', 0, 4), '-', substr('${key_5d}', 5, 2), '-', substr('${key_5d}', 7, 2)),1)        AS statDate,\n  d.tenantId,\n  d.shopId,\n  d.customerId,\n  if(d.cnt > 0, 1, 0)            AS lable\nFROM(\nSELECT\n  tenantId,\n  min(shopId)                    AS shopId,\n  customerId,\n  count(1)                       AS cnt\nFROM dws_alg_trd_pay_cust_1d\nWHERE (ds BETWEEN '${key_5d}' AND '${key_f1d}') AND isnotnull(customerId) \nGROUP BY tenantId, customerId) d;\n\n\n-------- 前六天 ---------\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_6d}')\nSELECT\n  date_add(concat(substr('${key_6d}', 0, 4), '-', substr('${key_6d}', 5, 2), '-', substr('${key_6d}', 7, 2)),1)        AS statDate,\n  d.tenantId,\n  d.shopId,\n  d.customerId,\n  if(d.cnt > 0, 1, 0)            AS lable\nFROM(\nSELECT\n  tenantId,\n  min(shopId)                    AS shopId,\n  customerId,\n  count(1)                       AS cnt\nFROM dws_alg_trd_pay_cust_1d\nWHERE (ds BETWEEN '${key_6d}' AND substr('${bdp.system.cyctime}',0,8)) AND isnotnull(customerId) \nGROUP BY tenantId, customerId) d;\n\n\n-------- 前七天 ---------\nINSERT OVERWRITE TABLE dws_alg_cust_label PARTITION (ds = '${key_7d}')\nSELECT\n  date_add(concat(substr('${key_7d}', 0, 4), '-', substr('${key_7d}', 5, 2), '-', substr('${key_7d}', 7, 2)),1)        AS statDate,\n  d.tenantId,\n  d.shopId,\n  d.customerId,\n  if(d.cnt > 0, 1, 0)            AS lable\nFROM(\nSELECT\n  tenantId,\n  min(shopId)                    AS shopId,\n  customerId,\n  count(1)                       AS cnt\nFROM dws_alg_trd_pay_cust_1d\nWHERE (ds BETWEEN '${key_7d}' AND '${bdp.system.bizdate}') AND isnotnull(customerId) \nGROUP BY tenantId, customerId) d;\n\n",
                                "taskParams": "##Number of CPU cores need driver program is running\n##driver.cores=1\n\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\n##driver.maxResultSize=1g\n\n##Driver number memory used by a process\n##driver.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##executor.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##logConf=spark.logConf",
                                "scheduleConf": "{\"beginDate\":\"2001-01-01\",\"min\":\"0\",\"periodType\":\"2\",\"hour\":\"4\",\"selfReliance\":0,\"endDate\":\"2121-01-01\"}",
                                "periodType": null,
                                "scheduleStatus": 1,
                                "submitStatus": 1,
                                "modifyUserId": 90,
                                "createUserId": 90,
                                "ownerUserId": 90,
                                "version": 2,
                                "nodePid": 1193,
                                "taskDesc": "",
                                "mainClass": "",
                                "exeArgs": "",
                                "flowId": 1050,
                                "createUser": {
                                    "id": 90,
                                    "gmtCreate": 1535092930000,
                                    "gmtModified": 1535092930000,
                                    "isDeleted": 0,
                                    "userName": "yizhou@dtstack.com",
                                    "phoneNumber": null,
                                    "dtuicUserId": 6999,
                                    "email": "",
                                    "status": 0,
                                    "defaultProjectId": null
                                },
                                "modifyUser": null,
                                "ownerUser": null,
                                "taskPeriodId": 2,
                                "taskPeriodType": "天任务",
                                "nodePName": null,
                                "readWriteLockVO": null,
                                "userId": null,
                                "lockVersion": null,
                                "taskVariables": null,
                                "forceUpdate": false,
                                "subNodes": null,
                                "relatedTasks": null,
                                "createModel": 0,
                                "operateModel": 0,
                                "pythonVersion": 0,
                                "learningType": 0,
                                "input": null,
                                "output": null,
                                "options": null,
                                "flowName": null,
                                "syncModel": 0,
                                "increColumn": null,
                                "taskVOS": null,
                                "subTaskVOS": null,
                                "resourceList": null,
                                "refResourceList": null,
                                "taskVersions": null,
                                "cron": "0 0 4 * * ?"
                            },
                            "id": 278715,
                            "gmtCreate": 1547647312000,
                            "gmtModified": 1547647312000,
                            "isDeleted": 0,
                            "tenantId": null,
                            "projectId": null,
                            "jobId": "2092f841",
                            "jobKey": "cronTrigger_1057_20190117040000",
                            "jobName": "cronJob_job_dws_alg_label_20190117040000",
                            "status": 18,
                            "taskId": 1057,
                            "createUserId": 90,
                            "type": 0,
                            "businessDate": "2019-01-16 ",
                            "cycTime": "2019-01-17 04:00:00",
                            "execStartTime": null,
                            "execEndTime": null,
                            "execTime": null,
                            "execStartDate": null,
                            "execEndDate": null,
                            "taskPeriodId": 2,
                            "taskPeriodType": "天任务",
                            "jobVOS": null,
                            "batchEngineJob": {
                                "id": 297213,
                                "gmtCreate": 1547647312000,
                                "gmtModified": 1547668804000,
                                "isDeleted": 0,
                                "status": 18,
                                "jobId": "2092f841",
                                "engineJobId": null,
                                "logInfo": "{ \"msg_info\": \"依赖任务链路存在任务处于冻结状态\"}",
                                "engineLog": "",
                                "execStartTime": null,
                                "execEndTime": null,
                                "execTime": 0
                            },
                            "subNodes": null,
                            "flowJobId": "58a98fff",
                            "relatedJobs": null,
                            "isDirty": 0
                        }
                    ],
                    "batchEngineJob": {
                        "id": 297207,
                        "gmtCreate": 1547647312000,
                        "gmtModified": 1547668804000,
                        "isDeleted": 0,
                        "status": 18,
                        "jobId": "1cb14eb7",
                        "engineJobId": null,
                        "logInfo": "{ \"msg_info\": \"依赖任务链路存在任务处于冻结状态\"}",
                        "engineLog": "",
                        "execStartTime": null,
                        "execEndTime": null,
                        "execTime": 0
                    },
                    "subNodes": null,
                    "flowJobId": "58a98fff",
                    "relatedJobs": null,
                    "isDirty": 0
                },
                "flowJobId": "0",
                "relatedJobs": null,
                "isDirty": 0
            },
            {
                "batchTask": {
                    "id": 964,
                    "gmtCreate": 1536584321000,
                    "gmtModified": 1536721490000,
                    "isDeleted": 0,
                    "tenantId": 129,
                    "projectId": 77,
                    "name": "job_ads_nrs_mp_consume_freq",
                    "taskType": 0,
                    "computeType": 1,
                    "engineType": 1,
                    "sqlText": "CREATE TABLE if NOT EXISTS ads_nrs_mp_consume_freq (\nid                      STRING      COMMENT '主键ID'\n,tenantId               STRING      COMMENT '租户ID'\n,freq                   BIGINT      COMMENT '消费次数'\n,days                   DOUBLE      COMMENT '达成该消费频次的平均天数'\n,gmt_created            STRING      COMMENT '创建时间'\n,gmt_modified           STRING      COMMENT '修改时间'\n) COMMENT '会员消费频次表'\nPARTITIONED BY (ds STRING COMMENT '时间分区') STORED AS orc LIFECYCLE 365;\n\nset spark.sql.shuffle.partitions=1;\n\nINSERT OVERWRITE TABLE ads_nrs_mp_consume_freq PARTITION (ds = '${bdp.system.bizdate}')\nSELECT\n  concat(res.tenantId, res.rank)  AS id,\n  res.tenantId                    AS tenantId,\n  res.rank                        AS freq,\n  avg(res.interDays)              AS days,\n  '${bdp.system.cyctime}'         AS gmt_created,\n  '${bdp.system.cyctime}'         AS gmt_modified\nFROM (\n       SELECT\n         total.tenantId,\n         total.memberId,\n         datediff(to_date(total.paymentTime), to_date(total.upgradeTime)) AS interDays,\n         total.rank\n       FROM (\n              SELECT\n                o.tenantId,\n                o.memberId,\n                o.paymentTime,\n                first(cust.upgradeTime) AS upgradeTime,\n                row_number()               over (partition BY o.tenantId, o.memberId ORDER BY o.paymentTime) rank\n              FROM dwd_ord_trd_d o\n                JOIN dim_cust_info cust\n                  ON o.tenantId = cust.tenantId AND o.customerId = cust.id AND cust.ds = '${bdp.system.bizdate}'\n              WHERE isnotnull(o.memberId) AND isnotnull(cust.upgradeTime) AND o.paymentTime >= cust.upgradeTime AND o.ds = '${bdp.system.bizdate}'\n              GROUP BY o.tenantId, o.memberId, o.paymentTime) total\n       WHERE total.rank <= 11) res\nGROUP BY res.tenantId, res.rank;\n",
                    "taskParams": "##Number of CPU cores need driver program is running\n##driver.cores=1\n\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\n##driver.maxResultSize=1g\n\n##Driver number memory used by a process\n##driver.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##executor.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##logConf=spark.logConf",
                    "scheduleConf": "{\"selfReliance\":0,\"min\":\"30\",\"hour\":\"4\",\"periodType\":\"2\",\"beginDate\":\"2001-01-01\",\"endDate\":\"2121-01-01\"}",
                    "periodType": null,
                    "scheduleStatus": 2,
                    "submitStatus": 1,
                    "modifyUserId": 90,
                    "createUserId": 90,
                    "ownerUserId": 90,
                    "version": 9,
                    "nodePid": 1169,
                    "taskDesc": "",
                    "mainClass": "",
                    "exeArgs": "",
                    "flowId": 0,
                    "createUser": {
                        "id": 90,
                        "gmtCreate": 1535092930000,
                        "gmtModified": 1535092930000,
                        "isDeleted": 0,
                        "userName": "yizhou@dtstack.com",
                        "phoneNumber": null,
                        "dtuicUserId": 6999,
                        "email": "",
                        "status": 0,
                        "defaultProjectId": null
                    },
                    "modifyUser": null,
                    "ownerUser": null,
                    "taskPeriodId": 2,
                    "taskPeriodType": "天任务",
                    "nodePName": null,
                    "readWriteLockVO": null,
                    "userId": null,
                    "lockVersion": null,
                    "taskVariables": null,
                    "forceUpdate": false,
                    "subNodes": null,
                    "relatedTasks": null,
                    "createModel": 0,
                    "operateModel": 0,
                    "pythonVersion": 0,
                    "learningType": 0,
                    "input": null,
                    "output": null,
                    "options": null,
                    "flowName": null,
                    "syncModel": 0,
                    "increColumn": null,
                    "taskVOS": null,
                    "subTaskVOS": null,
                    "resourceList": null,
                    "refResourceList": null,
                    "taskVersions": null,
                    "cron": "0 30 4 * * ?"
                },
                "id": 278816,
                "gmtCreate": 1547647311000,
                "gmtModified": 1547647311000,
                "isDeleted": 0,
                "tenantId": null,
                "projectId": null,
                "jobId": "8bb22589",
                "jobKey": "cronTrigger_964_20190117043000",
                "jobName": "cronJob_job_ads_nrs_mp_consume_freq_20190117043000",
                "status": 18,
                "taskId": 964,
                "createUserId": 90,
                "type": 0,
                "businessDate": "2019-01-16 ",
                "cycTime": "2019-01-17 04:30:00",
                "execStartTime": null,
                "execEndTime": null,
                "execTime": null,
                "execStartDate": null,
                "execEndDate": null,
                "taskPeriodId": 2,
                "taskPeriodType": "天任务",
                "jobVOS": null,
                "batchEngineJob": {
                    "id": 297314,
                    "gmtCreate": 1547647311000,
                    "gmtModified": 1547670608000,
                    "isDeleted": 0,
                    "status": 18,
                    "jobId": "8bb22589",
                    "engineJobId": null,
                    "logInfo": "{ \"msg_info\": \"当前任务处于冻结状态\"}",
                    "engineLog": "",
                    "execStartTime": null,
                    "execEndTime": null,
                    "execTime": 0
                },
                "subNodes": null,
                "flowJobId": "0",
                "relatedJobs": null,
                "isDirty": 0
            },
            {
                "batchTask": {
                    "id": 982,
                    "gmtCreate": 1536647022000,
                    "gmtModified": 1536987925000,
                    "isDeleted": 0,
                    "tenantId": 129,
                    "projectId": 77,
                    "name": "job_ads_nrs_ma_ord_source_stat",
                    "taskType": 0,
                    "computeType": 1,
                    "engineType": 1,
                    "sqlText": "CREATE TABLE if NOT EXISTS ads_nrs_ma_ord_source_stat(\nid                      STRING      COMMENT ''\n,tenantId               STRING      COMMENT '租户ID'\n,shopId                 STRING      COMMENT '门店ID'\n,stat_date              STRING      COMMENT '日期'\n,dimId                  STRING      COMMENT '维表ID'\n,cnt                    BIGINT      COMMENT '订单渠道来源数量'\n,percentVal             DOUBLE      COMMENT '订单渠道来源比例'\n,gmt_created            STRING      COMMENT '创建时间'\n,gmt_modified           STRING      COMMENT '修改时间'\n) COMMENT '历史订单渠道来源占比'\nPARTITIONED BY (ds STRING COMMENT '时间分区') STORED AS orc LIFECYCLE 365;\n\n\nset spark.sql.shuffle.partitions=1;\n\n\nINSERT OVERWRITE TABLE ads_nrs_ma_ord_source_stat PARTITION (ds = '${bdp.system.bizdate}')\nSELECT\n  concat(stat.tenantId,stat.shopId,'${bdp.system.bizdate}',stat.dimId)  AS id,\n  stat.tenantId,\n  stat.shopId,\n  concat(substr(${bdp.system.bizdate}, 0, 4), '-', substr(${bdp.system.bizdate}, 5, 2), '-',substr(${bdp.system.bizdate}, 7, 2)) AS stat_date,\n  stat.dimId,\n  stat.cnt,\n  stat.pVal,\n  '${bdp.system.cyctime}' AS gmt_created,\n  '${bdp.system.cyctime}' AS gmt_modified\nFROM (\n       SELECT\n         res.tenantId,\n         '0'                AS shopId,\n         res.dimId,\n         res.cnt            AS cnt,\n         res.cnt / pd_1.cnt AS pVal\n       FROM (\n              SELECT\n                pd.tenantId,\n                pd.dimId,\n                count(pd.dimId) AS cnt\n              FROM (\n                     SELECT\n                       tenantId,\n                       (CASE\n                        WHEN sourceChild IN (1, 2, 3, 141, 142)\n                          THEN 31\n                        WHEN sourceChild IN (31, 32, 33)\n                          THEN 32\n                        WHEN sourceChild = 161\n                          THEN 33\n                        WHEN sourceChild = 181\n                          THEN 34\n                        ELSE 35\n                        END) AS dimId\n                     FROM dwd_ord_trd_d\n                     WHERE ds = '${bdp.system.bizdate}' AND tradeStatus = 4) pd\n              GROUP BY pd.tenantId, pd.dimId) res LEFT OUTER JOIN (SELECT\n                                                                     tenantId,\n                                                                     count(sourceChild) AS cnt\n                                                                   FROM dwd_ord_trd_d\n                                                                   WHERE\n                                                                     ds = '${bdp.system.bizdate}'\n                                                                     AND\n                                                                     tradeStatus = 4\n                                                                   GROUP BY tenantId) pd_1 ON res.tenantId = pd_1.tenantId\n       UNION\n       SELECT\n         res.tenantId,\n         res.shopId,\n         res.dimId,\n         res.cnt            AS cnt,\n         res.cnt / pd_1.cnt AS pVal\n       FROM (\n              SELECT\n                pd.tenantId,\n                pd.shopId,\n                pd.dimId,\n                count(pd.dimId) AS cnt\n              FROM (\n                     SELECT\n                       tenantId,\n                       shopId,\n                       (CASE\n                        WHEN sourceChild IN (1, 2, 3, 141, 142)\n                          THEN 31\n                        WHEN sourceChild IN (31, 32, 33)\n                          THEN 32\n                        WHEN sourceChild = 161\n                          THEN 33\n                        WHEN sourceChild = 181\n                          THEN 34\n                        ELSE 35\n                        END) AS dimId\n                     FROM dwd_ord_trd_d\n                     WHERE ds = '${bdp.system.bizdate}' AND tradeStatus = 4) pd\n              GROUP BY pd.tenantId, pd.shopId, pd.dimId) res LEFT OUTER JOIN (SELECT\n                                                                                tenantId,\n                                                                                shopId,\n                                                                                count(sourceChild) AS cnt\n                                                                              FROM dwd_ord_trd_d\n                                                                              WHERE\n                                                                                ds = '${bdp.system.bizdate}'\n                                                                                AND\n                                                                                tradeStatus = 4\n                                                                              GROUP BY tenantId, shopId) pd_1\n           ON res.tenantId = pd_1.tenantId AND res.shopId = pd_1.shopId) stat;\n\n",
                    "taskParams": "##Number of CPU cores need driver program is running\n##driver.cores=1\n\n##Each Spark action (collect) limits the total size of the serialized results of all partitions. Setting value should not be less than 1M, 0 means no limit. If the total exceeds this limit, the program will terminate. Limit values may cause memory overflow error driver (dependent on spark. driver. memory and objects in the JVM's memory consumption).\n##driver.maxResultSize=1g\n\n##Driver number memory used by a process\n##driver.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##executor.memory=512m\n\n\n##Amount of memory used by each executor process. And JVM memory strings have the same format (for example, 512m, 2G)\n##logConf=spark.logConf",
                    "scheduleConf": "{\"selfReliance\":0,\"min\":\"30\",\"hour\":\"4\",\"periodType\":\"2\",\"beginDate\":\"2001-01-01\",\"endDate\":\"2121-01-01\"}",
                    "periodType": null,
                    "scheduleStatus": 2,
                    "submitStatus": 1,
                    "modifyUserId": 90,
                    "createUserId": 90,
                    "ownerUserId": 90,
                    "version": 13,
                    "nodePid": 1170,
                    "taskDesc": "",
                    "mainClass": "",
                    "exeArgs": "",
                    "flowId": 0,
                    "createUser": {
                        "id": 90,
                        "gmtCreate": 1535092930000,
                        "gmtModified": 1535092930000,
                        "isDeleted": 0,
                        "userName": "yizhou@dtstack.com",
                        "phoneNumber": null,
                        "dtuicUserId": 6999,
                        "email": "",
                        "status": 0,
                        "defaultProjectId": null
                    },
                    "modifyUser": null,
                    "ownerUser": null,
                    "taskPeriodId": 2,
                    "taskPeriodType": "天任务",
                    "nodePName": null,
                    "readWriteLockVO": null,
                    "userId": null,
                    "lockVersion": null,
                    "taskVariables": null,
                    "forceUpdate": false,
                    "subNodes": null,
                    "relatedTasks": null,
                    "createModel": 0,
                    "operateModel": 0,
                    "pythonVersion": 0,
                    "learningType": 0,
                    "input": null,
                    "output": null,
                    "options": null,
                    "flowName": null,
                    "syncModel": 0,
                    "increColumn": null,
                    "taskVOS": null,
                    "subTaskVOS": null,
                    "resourceList": null,
                    "refResourceList": null,
                    "taskVersions": null,
                    "cron": "0 30 4 * * ?"
                },
                "id": 278823,
                "gmtCreate": 1547647311000,
                "gmtModified": 1547647311000,
                "isDeleted": 0,
                "tenantId": null,
                "projectId": null,
                "jobId": "b8699f4d",
                "jobKey": "cronTrigger_982_20190117043000",
                "jobName": "cronJob_job_ads_nrs_ma_ord_source_stat_20190117043000",
                "status": 18,
                "taskId": 982,
                "createUserId": 90,
                "type": 0,
                "businessDate": "2019-01-16 ",
                "cycTime": "2019-01-17 04:30:00",
                "execStartTime": null,
                "execEndTime": null,
                "execTime": null,
                "execStartDate": null,
                "execEndDate": null,
                "taskPeriodId": 2,
                "taskPeriodType": "天任务",
                "jobVOS": null,
                "batchEngineJob": {
                    "id": 297321,
                    "gmtCreate": 1547647311000,
                    "gmtModified": 1547670609000,
                    "isDeleted": 0,
                    "status": 18,
                    "jobId": "b8699f4d",
                    "engineJobId": null,
                    "logInfo": "{ \"msg_info\": \"当前任务处于冻结状态\"}",
                    "engineLog": "",
                    "execStartTime": null,
                    "execEndTime": null,
                    "execTime": 0
                },
                "subNodes": null,
                "flowJobId": "0",
                "relatedJobs": null,
                "isDirty": 0
            }
        ],
        "batchEngineJob": {
            "id": 297042,
            "gmtCreate": 1547647311000,
            "gmtModified": 1547661625000,
            "isDeleted": 0,
            "status": 18,
            "jobId": "a9fa1f93",
            "engineJobId": null,
            "logInfo": "{ \"msg_info\": \"当前任务处于冻结状态\"}",
            "engineLog": "",
            "execStartTime": null,
            "execEndTime": null,
            "execTime": 0
        },
        "subNodes": null,
        "flowJobId": "0",
        "relatedJobs": null,
        "isDirty": 0
    },
    "space": 727
}
